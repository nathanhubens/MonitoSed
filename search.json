[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MonitoSed",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "MonitoSed"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "MonitoSed",
    "section": "Install",
    "text": "Install\npip install MonitoSed",
    "crumbs": [
      "MonitoSed"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "MonitoSed",
    "section": "How to use",
    "text": "How to use\nfrom monitosed.data.core import *\nfrom monitosed.models import *\nfrom monitosed.losses import *",
    "crumbs": [
      "MonitoSed"
    ]
  },
  {
    "objectID": "index.html#partners",
    "href": "index.html#partners",
    "title": "MonitoSed",
    "section": "Partners",
    "text": "Partners",
    "crumbs": [
      "MonitoSed"
    ]
  },
  {
    "objectID": "regression.stagernet_old.html",
    "href": "regression.stagernet_old.html",
    "title": "classification.cnn",
    "section": "",
    "text": "from monitosed.classification.core import *\nfrom monitosed.data.core import *\nfrom monitosed.models import *\n\nfrom fastcore.xtras import Path\nfrom tsai.all import *\n\n\nusers, labels = get_users_labels('../_data/Smarthy2_Behavioral.xlsx', drop_ixs=[0,2,7, 12, 13, 16])\n\n\npath = Path(\"../_data/foot1/Rest\"); path.ls()\n\n(#14) [Path('../_data/foot1/Rest/VR23_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR20_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR26_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR31_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR27_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR40_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR51_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR41_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR38_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR35_RestStim_data_clean.mat')...]\n\n\n\nmats = load_mats(path, 2)\n\n\n\n\nLoading: VR20_Reststim_data_clean.mat\nLoading: VR23_RestStim_data_clean.mat\n\n\n\n#Optional\nsignal_len = 1500\n\n\nx = prepare_train_data(mats)\n\n\nx.shape\n\n(112, 173, 1500)\n\n\n\nread_mats = [read_data(mat) for mat in mats]\n\n\ny = np.concatenate([np.repeat(labels.values[i], read_mats[i].shape[0]) for i in range(len(mats))])\n\n\ntrain_ix = int((1-0.2)*len(x))\n\n\nsplits = (np.arange(0, train_ix), np.arange(train_ix, x.shape[0]))\n\n\ntfms  = [None, [TSRegression()]]\nbatch_tfms = TSStandardize(by_sample=True, by_var=True)\n#dls = get_ts_dls(x, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=128)\ndls = get_ts_dls(x, y, splits=splits, tfms=tfms, bs=128)\n\n\nxb,yb = dls.one_batch(); xb,yb\n\n(TSTensor(samples:89, vars:173, len:1500, device=cuda:0, dtype=torch.float32),\n tensor([1.3000, 1.3000, 1.0000, 1.0000, 1.0000, 1.3000, 1.3000, 1.3000, 1.3000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.3000, 1.3000, 1.0000, 1.3000, 1.3000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.3000, 1.0000, 1.0000, 1.3000, 1.0000,\n         1.3000, 1.3000, 1.0000, 1.3000, 1.0000, 1.0000, 1.3000, 1.3000, 1.3000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.3000, 1.0000, 1.0000, 1.3000,\n         1.0000, 1.0000, 1.3000, 1.3000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.3000, 1.0000, 1.0000, 1.3000, 1.0000, 1.0000, 1.0000, 1.3000, 1.3000,\n         1.0000, 1.0000, 1.3000, 1.0000, 1.0000, 1.3000, 1.0000, 1.0000, 1.3000,\n         1.3000, 1.0000, 1.0000, 1.0000, 1.3000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.3000, 1.0000, 1.0000, 1.0000],\n        device='cuda:0'))\n\n\n\nplt.plot(xb[25][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nxb.shape\n\ntorch.Size([89, 173, 1500])\n\n\n\nnet = StagerNet(173, embed_dim=1)\n\n\nnet(xb.to('cpu'))\n\nTSTensor(vars:89, len:1, device=cpu, dtype=torch.float32)\n\n\n\nlearn = ts_learner(dls, net, loss_func=CustomLoss(), metrics=[mae, rmse], wd=0.05, cbs=ShowGraph())\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=5.248074739938602e-05)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(15, 1e-5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae\n_rmse\ntime\n\n\n\n\n0\n1.335700\n2.026015\n1.487038\n1.601559\n00:00\n\n\n1\n1.253563\n1.620306\n1.316288\n1.387200\n00:00\n\n\n2\n1.222349\n1.485492\n1.258669\n1.308554\n00:00\n\n\n3\n1.217683\n1.667337\n1.286582\n1.431115\n00:00\n\n\n4\n1.182587\n1.407716\n1.215789\n1.264770\n00:00\n\n\n5\n1.187551\n1.575417\n1.312858\n1.355720\n00:00\n\n\n6\n1.188602\n1.348244\n1.177107\n1.232632\n00:00\n\n\n7\n1.146437\n1.260941\n1.115324\n1.185984\n00:00\n\n\n8\n1.126039\n1.444867\n1.223265\n1.290918\n00:00\n\n\n9\n1.096403\n1.474271\n1.205359\n1.320297\n00:00\n\n\n10\n1.091534\n1.162826\n1.067966\n1.121466\n00:00\n\n\n11\n1.062970\n1.319969\n1.140115\n1.224673\n00:00\n\n\n12\n1.044469\n1.128299\n1.035776\n1.104908\n00:00\n\n\n13\n1.028936\n1.288494\n1.081119\n1.223057\n00:00\n\n\n14\n1.006987\n1.293191\n1.129485\n1.207020\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nye_valid, y_valid = learn.get_preds()\n\n\n\n\n\n\n\n\n\nye_valid\n\ntensor([[ 0.4309],\n        [ 0.1247],\n        [-0.0552],\n        [ 0.3877],\n        [ 0.2811],\n        [ 0.5449],\n        [ 0.5925],\n        [ 0.2351],\n        [-1.4151],\n        [ 0.2903],\n        [ 0.5751],\n        [-0.0276],\n        [ 0.4873],\n        [-0.1903],\n        [ 0.0373],\n        [ 0.7545],\n        [ 0.3498],\n        [ 0.0060],\n        [ 0.1857],\n        [ 0.6753],\n        [ 1.3156],\n        [ 0.4011],\n        [ 0.4180]])\n\n\n\ny_valid\n\ntensor([1.3000, 1.3000, 1.3000, 1.3000, 1.3000, 1.3000, 1.3000, 1.3000, 1.3000,\n        1.3000, 1.3000, 1.3000, 1.3000, 1.3000, 1.3000, 1.3000, 1.3000, 1.3000,\n        1.3000, 1.3000, 1.3000, 1.3000, 1.3000])"
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "preprocessing",
    "section": "",
    "text": "load_data\n\n load_data (data_path, save=False)\n\n\n\n\nEEG_preprocess\n\n EEG_preprocess (raw, label, get_raw=False)\n\n\n\n\nEEG_preprocess_eoec\n\n EEG_preprocess_eoec (raw, EC=True, get_raw=False)\n\nNous prenons un sujet comme référence pour la construction de notre DB:\nExemple:\n\nfile_path = ‘/home/JennebauffeC/pytorchVAE/fastAI/Monitosed/Biowin/sub-001/VR_1_VRH.vhdr’:\n\n\n\n\nget_preprocessed_EEG\n\n get_preprocessed_EEG (files)\n\n\n\n\npreprocess_files\n\n preprocess_files (file_paths, label_value)\n\nPlease respect the BIDS format\n\n\n\npreprocess_files_eoec\n\n preprocess_files_eoec (eeg_dir)\n\n\n\n\ncreate_binary_combinations\n\n create_binary_combinations (x1, x2, y1, y2, save_path_x, save_path_y)\n\n\n\n\ncreate_epochs_from_tensors\n\n create_epochs_from_tensors (data_path)\n\n\n\n\nget_ica\n\n get_ica (epochs, save_path, save=False)",
    "crumbs": [
      "preprocessing",
      "preprocessing"
    ]
  },
  {
    "objectID": "monitosed.html",
    "href": "monitosed.html",
    "title": "Monitosed",
    "section": "",
    "text": "from fastai.tabular.all import *\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom model import stagerNetAAE\nfrom utils import UnfreezeFcCrit\n\n# Set your paths\ngithub_path = 'fastAI/GitHub/' # path to GitHub Monitosed folder \neeg_dir = '/home/JennebauffeC/pytorchVAE/fastAI/data/LEMON/Preprocessed/' # path to public dataset\nbiowin_file_path = '/home/JennebauffeC/pytorchVAE/fastAI/Monitosed/Biowin/sub-001/VR_1_VRH.vhdr' # path to reference file for electrode positions\ndata_dir = '/home/JennebauffeC/pytorchVAE/fastAI/data/LEMON'\n\n# device = torch.device(torch.cuda.current_device())\ndev = torch.device('cuda:1')\ndevice = torch.device(dev if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.set_device(device)\nprint('the current device is: ', device)\n\nfrom model20 import stagerNetAAE\nfrom utils1 import LossAttrMetric, GetLatentSpace, norm_batch, UnfreezeFcCrit, SwitchAttribute, distrib_regul_regression, hist_lab, plot_results\nhyp_files = [file for file in eeg_dir.glob('*_HYP.mat')]\nrest_files = [file for file in eeg_dir.glob('*_Resting.mat')]\nvrh_files = [file for file in eeg_dir.glob('*_VRH.mat')]\nx_hyp, y_hyp = preprocess_files(hyp_files, 1)\nx_resting, y_resting = preprocess_files(rest_files, 0)\nx_vrh, y_vrh = preprocess_files(vrh_files, 2)",
    "crumbs": [
      "tutorial",
      "Monitosed"
    ]
  },
  {
    "objectID": "monitosed.html#train-the-auto-encoder-part",
    "href": "monitosed.html#train-the-auto-encoder-part",
    "title": "Monitosed",
    "section": "Train the Auto-Encoder part",
    "text": "Train the Auto-Encoder part\n\nae_filename = ''\nacc_factor = 1\nmodel = stagerNetAAE(latent_dim=64, channels=x.shape[1], timestamps=x.shape[-1], acc_factor=acc_factor, dropout_rate=.3)\n\nmetrics = [rmse]\nlearn = Learner(dls, model, loss_func = model.ae_loss_func, metrics=metrics, opt_func=ranger)\nlearning_rate = learn.lr_find()\nprint('learning rate: '+str(learning_rate.valley))\nlearn.fit_flat_cos(n_epoch=100, lr=learning_rate.valley, \n                   cbs=[\n                        GradientAccumulation(n_acc=dls.bs*acc_factor),\n                        TrackerCallback(),\n                        SaveModelCallback(fname=ae_filename),\n                        EarlyStoppingCallback(min_delta=1e-4,patience=10)])\n\nstate_dict = torch.load(f'models/{ae_filename}.pth') # load the best weights",
    "crumbs": [
      "tutorial",
      "Monitosed"
    ]
  },
  {
    "objectID": "monitosed.html#train-adversarial",
    "href": "monitosed.html#train-adversarial",
    "title": "Monitosed",
    "section": "Train Adversarial",
    "text": "Train Adversarial\n\nacc_factor = 1\nmodel = stagerNetAAE(latent_dim=64, channels=x.shape[1], timestamps=x.shape[-1],\n                     acc_factor=acc_factor, dropout_rate=0.5,\n                     adv_weight=0.9, recons_weight=0.1)\n# pretrained_filename = 'monitosed_pretrained_LEMON_aae_10s_shared'\n# state_dict = torch.load(f'models/{pretrained_filename}.pth') # load the best weights\nmodel.load_state_dict(state_dict, strict=False)\nmodel = model.to(device)\nmodel.gen_train = False\n\n# adv_filename = 'monitosed_BIOWIN_aae_REST_VRH_05_24_16h' #reference for adversarial\nadv_filename = 'monitosed_BIOWIN_aae_REST_VRH_05_24_17h_latent64'\n\nmetrics = [LossAttrMetric(\"recons_loss\"),\n           LossAttrMetric(\"discrim_loss\"),\n           LossAttrMetric(\"adv_loss\")]\nlearn = Learner(dls, model, loss_func=model.aae_loss_func_monitosed,\n               metrics=metrics, opt_func=ranger)\n\nlearning_rate = learn.lr_find()\nprint('learning rate: '+str(learning_rate.valley))\nlearn.fit_flat_cos(n_epoch=100, lr=learning_rate.valley,\n# learn.fit_flat_cos(50, lr=1e-2,\n                        cbs=[\n                            GradientAccumulation(n_acc=dls.bs*acc_factor),\n                            TrackerCallback(monitor='adv_loss'),\n#                             CustomSaveModelCallback(fname=adv_filename, monitor='adv_loss', start_at=2),\n                            SaveModelCallback(fname=adv_filename, monitor='adv_loss'),\n                            EarlyStoppingCallback(min_delta=1e-4,patience=20,monitor='adv_loss'),\n                            UnfreezeFcCrit(switch_every=2)])\n\nstate_dict = torch.load(f'models/{adv_filename}.pth') # load the best weights\n\n\nsave_path = ''\n\nz, target = extract_latent(state_dict, save_path, save)\nplot_results(z.to(device),target.cpu(),filename=save_path)",
    "crumbs": [
      "tutorial",
      "Monitosed"
    ]
  },
  {
    "objectID": "monitosed.html#train-classifier",
    "href": "monitosed.html#train-classifier",
    "title": "Monitosed",
    "section": "Train Classifier",
    "text": "Train Classifier\n\nacc_factor = 1\nmodel = stagerNetAAE(latent_dim=64, dropout_rate=.5, channels=x.shape[1],\n                     timestamps=x.shape[-1], acc_factor=acc_factor,\n                    recons_weight=.1, adv_weight=.4, classif_weight=.5)\n\n# pretrained_filename = 'monitosed_pretrained_LEMON_aae_10s_shared' # .pth of our best EC/EO model -&gt; rename both\n# pretrained_filename = 'monitosed_BIOWIN_aae_classif_REST_VRH_05_24_17h_latent64' # .pth of our best model -&gt; rename both\n# state_dict = torch.load(f'models/{pretrained_filename}.pth') # load the best weights\nmodel.load_state_dict(state_dict, strict=False)\nmodel = model.to(device)\n\nclassif_filename = ''\n\nmetrics = [LossAttrMetric(\"recons_loss\"),\n           LossAttrMetric(\"adv_loss\"),\n           LossAttrMetric(\"discrim_loss\"),\n           LossAttrMetric(\"classif_loss\")]\n\nlearn = Learner(dls, model, loss_func=model.aae_classif_loss_func_monitosed, metrics=metrics, opt_func=ranger, wd=1e-1)\n\nlearn.fit_flat_cos(100, lr=1e-2,\n                        cbs=[\n                            GradientAccumulation(n_acc=dls.bs*acc_factor),\n                            TrackerCallback(monitor='valid_loss'),\n                            SaveModelCallback(fname=classif_filename, monitor='valid_loss'),\n                            EarlyStoppingCallback(min_delta=1e-4,patience=20,monitor='valid_loss'),\n                            UnfreezeFcCrit(switch_every=5)])\n\nstate_dict = torch.load(f'models/{classif_filename}.pth') # load the best weights\n\n\nsave_path = ''\n\nz, target = extract_latent(state_dict, save_path, save)\nplot_results(z.to(device),target.cpu(),filename=save_path)",
    "crumbs": [
      "tutorial",
      "Monitosed"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "models",
    "section": "",
    "text": "stagerNetAAE\n\n stagerNetAAE (channels:int=23, timestamps:int=3001, acc_factor:int=8,\n               dropout_rate:float=0.5, level:int=0, latent_dim:int=128,\n               gan_depth:int=3, k_pool_size:int=13,\n               recons_weight:float=0.1, adv_weight:float=0.5,\n               classif_weight:float=0.4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "models",
      "models"
    ]
  },
  {
    "objectID": "monitosed_explanations.html",
    "href": "monitosed_explanations.html",
    "title": "Monitosed Explanations",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom fastai.basics import L, DataLoaders, Learner\n# from fastai.tabular.all import *\nimport numpy as np\nimport mne\nimport zarr\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom model20 import stagerNetAAE\nfrom utils1 import LossAttrMetric, GetLatentSpace, norm_batch, UnfreezeFcCrit, \\\n                SwitchAttribute, distrib_regul_regression, hist_lab, plot_results\n\ndevice = torch.device('cuda:1')\ntorch.cuda.set_device(device)\nprint('the current device is: ', device)\n\n# Define the channels to display\nchan_select = slice(0,17,2)\n\n# Get channel names\nfile_path = '/home/JennebauffeC/pytorchVAE/fastAI/Monitosed/Biowin/sub-001/VR_1_VRH.vhdr'\nmonitosed_VR1 = mne.io.read_raw_brainvision(file_path, preload=True)\nmontage = monitosed_VR1.get_montage()\n\nelectrode_info = montage.get_positions()['ch_pos']\nchan_names = list(electrode_info.keys())\nsliced_chan = chan_names[chan_select]\nprint('selected channels: ', sliced_chan)\n\n# Define frequency bands\nfrequency_bands = {\n    'alpha': (8, 13),\n    'beta': (13, 24),\n    'delta': (0.5, 4),\n    'theta': (4, 8)\n}\n\n[Errno 2] No such file or directory: 'fastAI/GitHub/'\n/home/JennebauffeC/pytorchVAE/fastAI/GitHub\nthe current device is:  cuda:1\nExtracting parameters from /home/JennebauffeC/pytorchVAE/fastAI/Monitosed/Biowin/sub-001/VR_1_VRH.vhdr...\nSetting channel info structure...\nReading 0 ... 924099  =      0.000 ...   924.099 secs...\n['Fp1', 'F3', 'F9', 'FC1', 'T7', 'CP1', 'P3', 'P9', 'Oz']\n# Sliding window function\ndef sliding_window(data, window_size, step_size):\n    print('in sliding windows, data shape: ', data.shape)\n    num_windows = (data.shape[-1] - window_size) // step_size + 1\n    windows = [data[:,:,i * step_size: i * step_size + window_size] for i in range(num_windows)]\n    return torch.stack(windows)\n\n# Compute power spectrum within each window\ndef compute_power_spectrum(data, nfft=64):\n    print('data shape: ', data.shape)\n    return torch.fft.fft(data.to('cpu'), n=nfft, dim=-1).abs().to(device) ** 2\n\n# Compute band power within each window\ndef compute_band_power(power_spectrum, frequency_bands, sampling_rate=50):\n    band_power = {}\n    for band_name, (low_freq, high_freq) in frequency_bands.items():\n        freq_indices = torch.where(\n            (torch.fft.fftfreq(power_spectrum.shape[-1], d=1 / sampling_rate) &gt;= low_freq) &\n            (torch.fft.fftfreq(power_spectrum.shape[-1], d=1 / sampling_rate) &lt;= high_freq)\n        )[0]\n        band_power[band_name] = power_spectrum[:, :, freq_indices].mean(dim=2)\n    return band_power\n\n# Sliding window analysis and statistical computation\ndef analyze_power_evolution(t, sorted_idx, window_size, step_size):\n    sliding_windows = sliding_window(t[sorted_idx], window_size, step_size)\n    power_spectra = compute_power_spectrum(sliding_windows)\n\n    band_power_over_time = {band: [] for band in frequency_bands}\n    for power_spectrum in power_spectra:\n        band_power = compute_band_power(power_spectrum, frequency_bands)\n        for band in frequency_bands:\n            band_power_over_time[band].append(band_power[band].mean(dim=0))\n\n    for band in frequency_bands:\n        band_power_over_time[band] = torch.stack(band_power_over_time[band])\n\n    return band_power_over_time\n\n# Normalize EEG data\ndef normalize_eeg_data(x):\n    eps = 1e-08\n    mean = torch.nanmean(x, dim=2)\n    std = torch.clamp_min(torch.std(x, dim=2), eps)\n    x_norm = (x - mean.unsqueeze(-1)) / torch.clamp_min(std.unsqueeze(-1), eps)\n    return torch.clamp(torch.clamp(x_norm, min=-3), max=3).float()\n\n# Plot band power over time\ndef plot_band_power_over_time(band_power_over_time, frequency_bands, channel_names, window_size, step_size):\n    num_windows = len(band_power_over_time[list(frequency_bands.keys())[0]])\n    time_axis = torch.arange(0, num_windows * step_size, step_size) + window_size // 2\n\n    for band, powers in band_power_over_time.items():\n        plt.figure(figsize=(12, 6))\n        sns.heatmap(powers.T.cpu().numpy(), xticklabels=time_axis.cpu().numpy(), yticklabels=channel_names, cmap='viridis')\n        plt.title(f'Power Evolution in {band.capitalize()} Band')\n        plt.xlabel('Time (samples)')\n        plt.ylabel('Channels')\n        plt.show()\n# Load required data\ntarget_pair = 'REST0_VRH2'\nclassif_filename = 'monitosed_BIOWIN_aae_classif_REST_VRH_05_24_17h_latent64'\n# classif_filename = 'monitosed_BIOWIN_aae_classif_REST_HYP_03_26_19h'\n# classif_filename = 'monitosed_BIOWIN_aae_classif_HYP_VRH_03_26_19h'\nstate_dict = torch.load(f'models/{classif_filename}.pth') # load the best weights\n\ndata_dir = f'/databases/monitosed-LUCA/Preprocessed_Data/{target_pair}'\nzarr_dir = f'/home/JennebauffeC/pytorchVAE/fastAI/Monitosed/preprocessed_data/{target_pair}'\n\nx_zarr = zarr.load(f'{data_dir}/x.zarr')\nica_zarr = zarr.open(f'{zarr_dir}/ica.zarr', mode='r')\nx = torch.tensor(ica_zarr, device=device).float()\nx_norm = normalize_eeg_data(x)\nt = normalize_eeg_data(torch.tensor(x_zarr, device=device).float())\n\ny = torch.load(f'{data_dir}/y.pt')\ny_tensor = torch.tensor([[0, 1] if lab == 0 else [1, 0] for lab in y], device=device).float()\n\nn_train_samples = int(x.shape[0]//4*3) # training set = 75% of the dataset\nn_total_samples = x.shape[0]\nsplits = (L(range(n_train_samples), use_list=True),\n          L(np.arange(n_train_samples, n_total_samples), use_list=True))\n\nds = TensorDataset(x_norm, y_tensor)\ntrain_ds = torch.utils.data.Subset(ds, splits[0])\nvalid_ds = torch.utils.data.Subset(ds, splits[1])\n\n# Create DataLoaders\ndls = DataLoaders.from_dsets(train_ds, valid_ds, bs=128, num_workers=0)\nprint('input device:', dls.train_ds[0][0].device)\nprint('label device:', dls.train_ds[0][1].device)\n\n# Define model\nmodel = stagerNetAAE(latent_dim=64, channels=x.shape[1], timestamps=x.shape[-1], \n                     dropout_rate=.3)\n\n# Extract the latent space\nmodel.load_state_dict(state_dict, strict=False)\nmodel = model.to(device)\n\n# Get latent space and targets\nlearn = Learner(dls, model, loss_func=model.aae_loss_func_monitosed)\nlearn.zi_valid = torch.tensor([]).to(device)\nlearn.labels = torch.tensor([]).to(device)\nlearn.get_preds(ds_idx=1,cbs=[GetLatentSpace(cycle_len=1)])\nz = learn.zi_valid\ntarget = torch.tensor(learn.labels)\nlearn.zi_valid = torch.tensor([]).to(device)\nlearn.labels = torch.tensor([]).to(device)\nlearn.get_preds(ds_idx=0,cbs=[GetLatentSpace(cycle_len=1)])\nz = torch.vstack((learn.zi_valid, z))\ntarget = torch.hstack((learn.labels, target))\nprint('latent space shape: ', z.shape)\n# Classification predictions\ny_pred = model.fc_clf_discr1(z)[:, 0]\nsorted_y, sorted_idx = y_pred.sort()\nsorted_y = (sorted_y-sorted_y.min())/(sorted_y.max()-sorted_y.min())\nmask = (sorted_y &gt; 0.2) & (sorted_y &lt; 0.8)\nsorted_y = sorted_y[mask]\nsorted_idx = sorted_idx[mask]\nsorted_y_tensor = y_tensor[sorted_idx,0]\n# Analyze power evolution using absolute differences\nwindow_size = 64  # Example window size\nstep_size = 4     # Example step size\nband_power_over_time = analyze_power_evolution(t[:,chan_select], sorted_idx, window_size, step_size)\n\n# Compute differences in power for each band\ndef compute_diffs(band_power_over_time, sorted_idx):\n    bdiff = []\n    target_chan = []\n    distances = [torch.tensor(0)]\n    band_idx = 0\n    for band in frequency_bands:\n        tmp_diff = torch.zeros(band_power_over_time[band].shape[1]).to(device)\n        for dist in np.arange(1, int(.9 * band_power_over_time[band].shape[0])):\n            if band_idx==0:\n                distances.append(torch.tensor(dist))\n            bpow0 = band_power_over_time[band][:-dist]\n            bpow1 = band_power_over_time[band][dist:]\n            mean_diff = (bpow1 - bpow0).mean(dim=0)\n            tmp_diff = torch.vstack((tmp_diff, mean_diff))\n        chan_idx = torch.argmax(tmp_diff.var(dim=0))\n        target_chan.append(sliced_chan[chan_idx])\n        bdiff.append(tmp_diff[:, chan_idx])\n        band_idx += 1\n    return torch.stack(distances), torch.stack(bdiff).T, target_chan\n\ndistances, bdiff, target_chan = compute_diffs(band_power_over_time, sorted_idx)\n\n# Plot results\nxax = distances/distances.max()\nlegend_list = [f\"{band} band of channel {ch}\" for band, ch in zip(frequency_bands.keys(), target_chan)]\n\nplt.figure(figsize=(12, 6))\nfor i, band_diff in enumerate(bdiff.T):\n    plt.plot(xax.cpu().numpy(), band_diff.cpu().numpy(), linewidth=3, label=legend_list[i])\nplt.xlabel('Distance Across Sorted Samples', fontsize=16)\nplt.ylabel('Power Difference (high score - low score)', fontsize=16)\nplt.title('Frequency Band Effect of Increasing Distance (REST vs VRH)', fontsize=18)\nplt.legend()\nplt.show()\n\nin sliding windows, data shape:  torch.Size([4252, 9, 500])\ndata shape:  torch.Size([110, 4252, 9, 64])",
    "crumbs": [
      "tutorial",
      "Monitosed Explanations"
    ]
  },
  {
    "objectID": "monitosed_explanations.html#old-code",
    "href": "monitosed_explanations.html#old-code",
    "title": "Monitosed Explanations",
    "section": "Old Code",
    "text": "Old Code\n\n# # Compute power differences\n# cpow_pool, tpow = compute_power_diff(t, sorted_idx, device)\n\n# Compute mean power and most variable channels for frequency bands\nmean_power_and_channel = compute_mean_power_and_channel(t[sorted_idx, :17, :])\nmean_band_pow = torch.stack([mean_power_and_channel[band]['mean_power'] for band in frequency_bands.keys()], dim=2).to(device)\n\n# Compute power differences for each frequency band\nbdiff = []\nfor i in range(len(frequency_bands)):\n    tmp_diff = torch.zeros(17).to(device)\n    for dist in np.arange(1, int(.51 * len(sorted_idx))):\n        bpow0 = mean_band_pow[:-dist, :, i]\n        bpow1 = mean_band_pow[dist:, :, i]\n        mean_diff = (bpow1 - bpow0).mean(dim=0)\n        tmp_diff = torch.vstack((tmp_diff, mean_diff))\n    bdiff.append(tmp_diff[:, torch.argmax(tmp_diff.var(dim=0))])\n\n# Plot results\nxax = torch.linspace(0, 1, len(bdiff[0]))\nsns.set(rc={'figure.figsize': (11.7, 8.27)})\nlegend_list = [f\"{band} band\" for band in frequency_bands.keys()]\nfor i, band_diff in enumerate(bdiff.T):\n    plt.plot(xax.cpu().numpy(), band_diff.cpu().numpy(), linewidth=3, label=legend_list[i])\nplt.xlabel('Distance Across Sorted Samples', fontsize=16)\nplt.ylabel('Power Difference (high score - low score)', fontsize=16)\nplt.title('Frequency Band Effect of Increasing Distance (REST vs VRH)', fontsize=18)\nplt.legend([f\"{band} band\" for band in frequency_bands.keys()])\nplt.show()\n\nAttributeError: 'list' object has no attribute 'T'\n\n\n\n# Function to normalize EEG data\ndef normalize_eeg_data(x):\n    eps = 1e-08\n    mean = torch.nanmean(x, dim=2)\n    std = torch.clamp_min(torch.std(x, dim=2), eps)\n    x_norm = (x - mean.unsqueeze(-1)) / torch.clamp_min(std.unsqueeze(-1), eps)\n    return torch.clamp(torch.clamp(x_norm, min=-3), max=3).float()\n\n# Function to compute power differences\ndef compute_power_diff(t, sorted_idx, device, win_size=50*2):\n    cpow = torch.square(t[sorted_idx])\n    pool_test = nn.MaxPool2d(kernel_size=(1, win_size), stride=(1, win_size // 2), padding=(0, win_size // 2)).to('cpu')\n    cpow_pool = pool_test(cpow.unsqueeze(1))[:, 0]\n    \n    eps = 1e-08\n    t_max, t_min = cpow_pool.max(dim=2).values.max(dim=0).values, cpow_pool.min(dim=2).values.min(dim=0).values\n    cpow_pool = (cpow_pool - t_min) / torch.clamp_min((t_max - t_min), eps)\n    \n    tpow = (torch.pow(t[sorted_idx], 2).mean(dim=2) - t_min) / torch.clamp_min((t_max - t_min), eps)\n    \n    return cpow_pool.to(device), tpow.to(device)\n\n# Function to compute mean power and most variable channels for frequency bands\ndef compute_mean_power_and_channel(t, nfft=128):\n    power_spectrum = torch.fft.fft(t.to('cpu'), n=nfft, dim=2).abs() ** 2\n    mean_power_and_channel = {}\n\n    for band_name, (low_freq, high_freq) in frequency_bands.items():\n        freq_indices = torch.where((torch.fft.fftfreq(nfft, d=1 / 50) &gt;= low_freq) &\n                                   (torch.fft.fftfreq(nfft, d=1 / 50) &lt;= high_freq))[0]\n        band_power = power_spectrum[:, :, freq_indices].mean(dim=2)\n        sorted_band_power, _ = band_power.sort(dim=0)\n        percentile_25_idx = int(0.25 * sorted_band_power.shape[0])\n        percentile_75_idx = int(0.75 * sorted_band_power.shape[0])\n        percentile_diff = sorted_band_power[percentile_75_idx] - sorted_band_power[percentile_25_idx]\n        max_var_channel_idx = torch.argmax(percentile_diff)\n        mean_power_and_channel[band_name] = {'mean_power': band_power, 'max_var_channel_idx': max_var_channel_idx}\n\n    return mean_power_and_channel",
    "crumbs": [
      "tutorial",
      "Monitosed Explanations"
    ]
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "training",
    "section": "",
    "text": "extract_latent\n\n extract_latent (state_dict, save_path, save)",
    "crumbs": [
      "training",
      "training"
    ]
  },
  {
    "objectID": "generation.cnn.vr_rest_old.html",
    "href": "generation.cnn.vr_rest_old.html",
    "title": "generation.cnn.vr_rest",
    "section": "",
    "text": "from monitosed.data.core import *\nfrom monitosed.generation.core import *\nimport mat73\nfrom fastcore.xtras import Path\nimport pandas as pd\nimport numpy as np\n\nfrom tsai.all import *"
  },
  {
    "objectID": "generation.cnn.vr_rest_old.html#load-data",
    "href": "generation.cnn.vr_rest_old.html#load-data",
    "title": "generation.cnn.vr_rest",
    "section": "Load data",
    "text": "Load data\n\nfrom tqdm.notebook import tqdm\nimport mat73\nimport scipy\n\n\ndef load_mats(path, max_len=None):\n    mats = []\n    length = ifnone(max_len, len(path.ls()))\n    for mat in tqdm(sorted(path.ls())[:length]):\n        try: \n            print(f\"Loading: {mat.name}\")\n            mats.append(mat73.loadmat(mat))\n        except:\n            print(f\"Loading: {mat.name}\")\n            mats.append(scipy.io.loadmat(mat))\n    return mats\n\n\nmats_rest = load_mats(path_rest)\nmats_vr = load_mats(path_vr)\n\n\n\n\nLoading: VR20_Reststim_data_clean.mat\nLoading: VR23_RestStim_data_clean.mat\nLoading: VR26_Reststim_data_clean.mat\nLoading: VR27_Reststim_data_clean.mat\nLoading: VR31_Reststim_data_clean.mat\nLoading: VR35_RestStim_data_clean.mat\nLoading: VR38_RestStim_data_clean.mat\nLoading: VR40_RestStim_data_clean.mat\nLoading: VR41_RestStim_data_clean.mat\nLoading: VR51_RestStim_data_clean.mat\nLoading: VR52_RestStim_data_clean.mat\nLoading: VR57_Reststim_data_clean.mat\nLoading: VR59_Reststim_data_clean.mat\nLoading: VR60_Reststim_data_clean.mat\nLoading: VR20_VRstim_data_clean.mat\nLoading: VR23_VRStim_data_clean.mat\nLoading: VR26_VRStim_data_clean.mat\nLoading: VR27_VRstim_data_clean.mat\nLoading: VR31_VRstim_data_clean.mat\nLoading: VR35_VRStim_data_clean.mat\nLoading: VR38_VRStim_data_clean.mat\nLoading: VR40_VRstim_data_clean.mat\nLoading: VR41_VRStim_data_clean.mat\nLoading: VR51_VRStim_data_clean.mat\nLoading: VR52_VRStim_data_clean.mat\nLoading: VR57_VRstim_data_clean.mat\nLoading: VR59_VRstim_data_clean.mat\nLoading: VR60_VRstim_data_clean.mat\n\n\n\n\n\n\n#Optional\nsignal_len = 1000"
  },
  {
    "objectID": "generation.cnn.vr_rest_old.html#get-labels",
    "href": "generation.cnn.vr_rest_old.html#get-labels",
    "title": "generation.cnn.vr_rest",
    "section": "Get labels",
    "text": "Get labels\nThe first idea is to perform forecasting, i.e. predict the future signal. To do so, we have to cut the measured signal into a past and the future we would like to predict.\n/! the stimulus happens at 250\n\ntrn_len = 1000\npred_len = signal_len-trn_len"
  },
  {
    "objectID": "generation.cnn.vr_rest_old.html#datablock",
    "href": "generation.cnn.vr_rest_old.html#datablock",
    "title": "generation.cnn.vr_rest",
    "section": "DataBlock",
    "text": "DataBlock\n\ngetters = [ItemGetter(0), ItemGetter(1)]\n\n\ndata = np.concatenate([read_data(mat) for mat in mats_rest])\ndata = data[:,:,:signal_len]\ndata.shape\n\n(778, 173, 1000)\n\n\n\ndata = np.stack([read_data(mat).mean(0) for mat in mats_rest])\ndata = data[:,:signal_len]\ndata.shape\n\n(14, 173, 1500)\n\n\n\nplt.plot(data[2][0])\nplt.plot(labels[2][0])\n\n\n\n\n\n\n\n\n\nlabels = np.concatenate([read_data(mat) for mat in mats_vr])\nlabels = labels[:,:,:signal_len]\nlabels.shape\n\n(795, 173, 1000)\n\n\n\nlabels = np.stack([read_data(mat).mean(0) for mat in mats_vr])\nlabels = labels[:,:signal_len]\nlabels.shape\n\n(14, 173, 1500)\n\n\n\nread_data(mats_rest[0]).shape\n\n(57, 173, 1500)\n\n\n\nx = torch.from_numpy(np.concatenate(data, axis=0)).unsqueeze(1)\ny = torch.from_numpy(np.concatenate(labels, axis=0)).unsqueeze(1)\nx.shape, y.shape\n\n(torch.Size([2422, 1, 1500]), torch.Size([2422, 1, 1500]))\n\n\n\ntrain_ix = int((1-valid_pct)*len(mats))\nread_mats = [read_data(mat) for mat in mats]\nrearranged_mats = [new_rearrange(mat) for mat in read_mats]\nn_train_sample = np.concatenate(rearranged_mats[:train_ix]).shape[0]\nreturn (np.arange(0, n_train_sample), np.arange(n_train_sample, np.concatenate(rearranged_mats).shape[0]))\n\n\nsplits = create_splits(mats_rest)\n\n\nsplits = (np.arange(0, 11), np.arange(11, 14))\n\n\nsplits\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([11, 12, 13]))\n\n\n\ndls = get_ts_dls(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), splits=splits, bs=128)\n\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([11, 1, 1500]), torch.Size([11, 1, 1500]))\n\n\n\nplt.plot(xb[0][0].to('cpu').numpy())\nplt.plot(yb[0][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nclass CustomLoss(nn.Module):\n    def __init__(self, size_average=None, reduce=None, alpha=0.5, reduction: str = 'mean') -&gt; None:\n        super(CustomLoss, self).__init__()\n        store_attr()\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        l1 = F.l1_loss(input, target, reduction=self.reduction)\n        mse = F.mse_loss(input, target, reduction=self.reduction)\n        return self.alpha*l1 + (1-self.alpha)*mse\n\n\nyb.shape\n\ntorch.Size([11, 1, 1500])\n\n\n\nmodel = TimeSeriesModel(1, yb.shape[2])\nlearn = Learner(dls, model, loss_func=CustomLoss(), metrics=[mae, rmse], wd=10, cbs=ShowGraph())\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=9.120108734350652e-05)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(150, lr_max=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae\n_rmse\ntime\n\n\n\n\n0\n1.299301\n1.017650\n0.723258\n1.145495\n00:00\n\n\n1\n1.258223\n1.015414\n0.721979\n1.144166\n00:00\n\n\n2\n1.226679\n1.014164\n0.721362\n1.143299\n00:00\n\n\n3\n1.201110\n1.014524\n0.721896\n1.143373\n00:00\n\n\n4\n1.179566\n1.013083\n0.721242\n1.142406\n00:00\n\n\n5\n1.160073\n1.007014\n0.717782\n1.138561\n00:00\n\n\n6\n1.148064\n1.002438\n0.715046\n1.135702\n00:00\n\n\n7\n1.139866\n0.995614\n0.711092\n1.131462\n00:00\n\n\n8\n1.131695\n0.993450\n0.710330\n1.129906\n00:00\n\n\n9\n1.123435\n0.996208\n0.713233\n1.130965\n00:00\n\n\n10\n1.115965\n0.981732\n0.706861\n1.120673\n00:00\n\n\n11\n1.109448\n0.958713\n0.694856\n1.104808\n00:00\n\n\n12\n1.102018\n0.946586\n0.688475\n1.096406\n00:00\n\n\n13\n1.095197\n1.019580\n0.731628\n1.142808\n00:00\n\n\n14\n1.087555\n1.001774\n0.721992\n1.132035\n00:00\n\n\n15\n1.077279\n0.937915\n0.688387\n1.089492\n00:00\n\n\n16\n1.068594\n0.800658\n0.621139\n0.988102\n00:00\n\n\n17\n1.061121\n0.819594\n0.622697\n1.003655\n00:00\n\n\n18\n1.051660\n0.804017\n0.612477\n0.993289\n00:00\n\n\n19\n1.040679\n0.746105\n0.579242\n0.950914\n00:00\n\n\n20\n1.028393\n0.737224\n0.571697\n0.947603\n00:00\n\n\n21\n1.016082\n0.697171\n0.549074\n0.918918\n00:00\n\n\n22\n1.002781\n0.715865\n0.563512\n0.932540\n00:00\n\n\n23\n0.988652\n0.552900\n0.472588\n0.797109\n00:00\n\n\n24\n0.974076\n2.684968\n1.465792\n1.966433\n00:00\n\n\n25\n0.958212\n0.857588\n0.647996\n1.028118\n00:00\n\n\n26\n0.942364\n6.556168\n2.388410\n3.275557\n00:00\n\n\n27\n0.928555\n6.010875\n2.474461\n3.086994\n00:00\n\n\n28\n0.914212\n0.919987\n0.689771\n1.076384\n00:00\n\n\n29\n0.900598\n0.573817\n0.499231\n0.804232\n00:00\n\n\n30\n0.891188\n0.405629\n0.422155\n0.640827\n00:00\n\n\n31\n0.881536\n0.430032\n0.437541\n0.650404\n00:00\n\n\n32\n0.867537\n0.370618\n0.396334\n0.578724\n00:00\n\n\n33\n0.856232\n2.943511\n1.549018\n2.091185\n00:00\n\n\n34\n0.846125\n3.482603\n1.757311\n2.263476\n00:00\n\n\n35\n0.834767\n9.473005\n2.963549\n3.974808\n00:00\n\n\n36\n0.823964\n14.158696\n2.785887\n5.032480\n00:00\n\n\n37\n0.814856\n3.706322\n2.044013\n2.309013\n00:00\n\n\n38\n0.809695\n10.222559\n3.345434\n4.183214\n00:00\n\n\n39\n0.801078\n3.775189\n1.898039\n2.333385\n00:00\n\n\n40\n0.791567\n1.368609\n1.088956\n1.277575\n00:00\n\n\n41\n0.784252\n0.469283\n0.507975\n0.658018\n00:00\n\n\n42\n0.776464\n0.392174\n0.421737\n0.607453\n00:00\n\n\n43\n0.768908\n0.361280\n0.391794\n0.573113\n00:00\n\n\n44\n0.761990\n0.341300\n0.368734\n0.559229\n00:00\n\n\n45\n0.754644\n0.338397\n0.367189\n0.561940\n00:00\n\n\n46\n0.748532\n0.334776\n0.359335\n0.547316\n00:00\n\n\n47\n0.741764\n0.339404\n0.362777\n0.560396\n00:00\n\n\n48\n0.734906\n0.338133\n0.363922\n0.564154\n00:00\n\n\n49\n0.728242\n0.345272\n0.367059\n0.571316\n00:00\n\n\n50\n0.721405\n0.359911\n0.380093\n0.582484\n00:00\n\n\n51\n0.715890\n3.187883\n1.513592\n2.201920\n00:00\n\n\n52\n0.709413\n19.323849\n3.629069\n5.931134\n00:00\n\n\n53\n0.703728\n0.337647\n0.360615\n0.563299\n00:00\n\n\n54\n0.697707\n0.351220\n0.370750\n0.577266\n00:00\n\n\n55\n0.691957\n0.375552\n0.403407\n0.593102\n00:00\n\n\n56\n0.686324\n0.359031\n0.379468\n0.584025\n00:00\n\n\n57\n0.681012\n0.356424\n0.371321\n0.585718\n00:00\n\n\n58\n0.676354\n0.353348\n0.369257\n0.582729\n00:00\n\n\n59\n0.671542\n0.360544\n0.376929\n0.588039\n00:00\n\n\n60\n0.666387\n0.360036\n0.376399\n0.586907\n00:00\n\n\n61\n0.662081\n0.347083\n0.365891\n0.573729\n00:00\n\n\n62\n0.658030\n0.350275\n0.367510\n0.577928\n00:00\n\n\n63\n0.653528\n0.347404\n0.365505\n0.575121\n00:00\n\n\n64\n0.649135\n0.345062\n0.364712\n0.571728\n00:00\n\n\n65\n0.644888\n0.336439\n0.358055\n0.562059\n00:00\n\n\n66\n0.640819\n0.337560\n0.359627\n0.562160\n00:00\n\n\n67\n0.636773\n0.341842\n0.364520\n0.565239\n00:00\n\n\n68\n0.632877\n0.345887\n0.366593\n0.570551\n00:00\n\n\n69\n0.629133\n0.345524\n0.363466\n0.572730\n00:00\n\n\n70\n0.625658\n0.350955\n0.366173\n0.579614\n00:00\n\n\n71\n0.622175\n0.353335\n0.368326\n0.581763\n00:00\n\n\n72\n0.618605\n0.353628\n0.368791\n0.581896\n00:00\n\n\n73\n0.615254\n0.350779\n0.365767\n0.579621\n00:00\n\n\n74\n0.612172\n0.348024\n0.363526\n0.576782\n00:00\n\n\n75\n0.608875\n0.343143\n0.361297\n0.570250\n00:00\n\n\n76\n0.605701\n0.339032\n0.358983\n0.565036\n00:00\n\n\n77\n0.602523\n0.335442\n0.356965\n0.560425\n00:00\n\n\n78\n0.599567\n52.053436\n7.824964\n9.811817\n00:00\n\n\n79\n0.596779\n0.345568\n0.362967\n0.572771\n00:00\n\n\n80\n0.593803\n0.343511\n0.361802\n0.570190\n00:00\n\n\n81\n0.590962\n292.014709\n18.417253\n23.782608\n00:00\n\n\n82\n0.588105\n293.192688\n17.970638\n23.842102\n00:00\n\n\n83\n0.585429\n0.318886\n0.349878\n0.537413\n00:00\n\n\n84\n0.582799\n0.329727\n0.353536\n0.553637\n00:00\n\n\n85\n0.580264\n0.335279\n0.355733\n0.561326\n00:00\n\n\n86\n0.577709\n0.405445\n0.422905\n0.623114\n00:00\n\n\n87\n0.575320\n5.950220\n2.102771\n3.127086\n00:00\n\n\n88\n0.572908\n79.104721\n8.209189\n12.242878\n00:00\n\n\n89\n0.570688\n0.583856\n0.495737\n0.822348\n00:00\n\n\n90\n0.568716\n0.355095\n0.368391\n0.585000\n00:00\n\n\n91\n0.566530\n0.353138\n0.366771\n0.582920\n00:00\n\n\n92\n0.564377\n0.353728\n0.367086\n0.583512\n00:00\n\n\n93\n0.562273\n0.348275\n0.363287\n0.577407\n00:00\n\n\n94\n0.560274\n0.348354\n0.363653\n0.577231\n00:00\n\n\n95\n0.558281\n0.352818\n0.367719\n0.581428\n00:00\n\n\n96\n0.556367\n0.361564\n0.376456\n0.589078\n00:00\n\n\n97\n0.554535\n0.354573\n0.369715\n0.582829\n00:00\n\n\n98\n0.552740\n207.875122\n15.748264\n19.999113\n00:00\n\n\n99\n0.550939\n0.727575\n0.613416\n0.917282\n00:00\n\n\n100\n0.549121\n0.449450\n0.489551\n0.639457\n00:00\n\n\n101\n0.547398\n1.002707\n0.755015\n1.117518\n00:00\n\n\n102\n0.545631\n0.363296\n0.378168\n0.590012\n00:00\n\n\n103\n0.543934\n535.617859\n26.717255\n32.317467\n00:00\n\n\n104\n0.542293\n0.411261\n0.410266\n0.640823\n00:00\n\n\n105\n0.540655\n0.381312\n0.388425\n0.611153\n00:00\n\n\n106\n0.539131\n0.369158\n0.380665\n0.597801\n00:00\n\n\n107\n0.537601\n0.364720\n0.378244\n0.592530\n00:00\n\n\n108\n0.536048\n0.365995\n0.379692\n0.593320\n00:00\n\n\n109\n0.534614\n0.366753\n0.379496\n0.594836\n00:00\n\n\n110\n0.533135\n0.364949\n0.377070\n0.593912\n00:00\n\n\n111\n0.531748\n0.359695\n0.372092\n0.589401\n00:00\n\n\n112\n0.530377\n0.342307\n0.362188\n0.568597\n00:00\n\n\n113\n0.529005\n0.352479\n0.365951\n0.582329\n00:00\n\n\n114\n0.527717\n0.354641\n0.367223\n0.584691\n00:00\n\n\n115\n0.526419\n0.353655\n0.366550\n0.583328\n00:00\n\n\n116\n0.525128\n0.351254\n0.365145\n0.580477\n00:00\n\n\n117\n0.523867\n0.348317\n0.363499\n0.577082\n00:00\n\n\n118\n0.522610\n0.345766\n0.362206\n0.573952\n00:00\n\n\n119\n0.521395\n0.343867\n0.361349\n0.571540\n00:00\n\n\n120\n0.520199\n0.342780\n0.361077\n0.570028\n00:00\n\n\n121\n0.519054\n0.341788\n0.360746\n0.568650\n00:00\n\n\n122\n0.517948\n0.340971\n0.360423\n0.567502\n00:00\n\n\n123\n0.516865\n0.340661\n0.360431\n0.567002\n00:00\n\n\n124\n0.515796\n0.340025\n0.360110\n0.566148\n00:00\n\n\n125\n0.514733\n0.339685\n0.359925\n0.565718\n00:00\n\n\n126\n0.513681\n0.339166\n0.359564\n0.565151\n00:00\n\n\n127\n0.512676\n0.338888\n0.359298\n0.564931\n00:00\n\n\n128\n0.511653\n0.338551\n0.358970\n0.564645\n00:00\n\n\n129\n0.510670\n0.338529\n0.358871\n0.564791\n00:00\n\n\n130\n0.509719\n0.338255\n0.358602\n0.564544\n00:00\n\n\n131\n0.508790\n0.338192\n0.358519\n0.564659\n00:00\n\n\n132\n0.507888\n0.338087\n0.358408\n0.564732\n00:00\n\n\n133\n0.506987\n0.338086\n0.358363\n0.564845\n00:00\n\n\n134\n0.506119\n0.337958\n0.358238\n0.564723\n00:00\n\n\n135\n0.505267\n0.337719\n0.358037\n0.564449\n00:00\n\n\n136\n0.504459\n0.337478\n0.357850\n0.564143\n00:00\n\n\n137\n0.503630\n0.337290\n0.357705\n0.563875\n00:00\n\n\n138\n0.502850\n0.337115\n0.357570\n0.563616\n00:00\n\n\n139\n0.502065\n0.336944\n0.357449\n0.563367\n00:00\n\n\n140\n0.501306\n0.336740\n0.357309\n0.563082\n00:00\n\n\n141\n0.500547\n0.336555\n0.357180\n0.562814\n00:00\n\n\n142\n0.499798\n0.336417\n0.357083\n0.562609\n00:00\n\n\n143\n0.499095\n0.336318\n0.357013\n0.562461\n00:00\n\n\n144\n0.498389\n0.336226\n0.356945\n0.562328\n00:00\n\n\n145\n0.497687\n0.336172\n0.356904\n0.562245\n00:00\n\n\n146\n0.497021\n0.336132\n0.356875\n0.562187\n00:00\n\n\n147\n0.496392\n0.336107\n0.356857\n0.562150\n00:00\n\n\n148\n0.495728\n0.336092\n0.356848\n0.562131\n00:00\n\n\n149\n0.495142\n0.336088\n0.356845\n0.562124\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nin_valid, pred_valid, true_valid = learn.get_preds(with_input=True)\n\n\n\n\n\n\n\n\n\n#plt.plot(in_valid[2][0].to('cpu').numpy())\nplt.plot(pred_valid[].to('cpu').numpy())\nplt.plot(true_valid[2][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\ninf = read_data(mats_rest[0])\n\n\nin_valid.shape\n\ntorch.Size([3, 1, 1500])\n\n\n\ninf[1][0].max()\n\n29.977590216479552\n\n\n\ntorch.tensor(inf[0][0], dtype=torch.float32).unsqueeze(0).unsqueeze(0).shape\n\ntorch.Size([1, 1, 1500])\n\n\n\ntorch.tensor(inf[0][0], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to('cuda:0')\n\ntensor([[[ 0.4247,  0.9331,  1.5741,  ..., -0.8875, -2.1496, -3.0908]]],\n       device='cuda:0')\n\n\n\ntest_probas, test_targets, test_preds = learn.get_X_preds(torch.tensor(inf[1][0], dtype=torch.float32).unsqueeze(0).unsqueeze(0), with_decoded=True)\n\n\n\n\n\n\n\n\n\nplt.plot(test_preds[0])\n\n\n\n\n\n\n\n\n\nlearn.model(torch.tensor(inf[0][0], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to('cuda:0'))\n\ntensor([[-26.7151,  18.9569, -21.5338,  ...,  42.8832,   9.8944,  10.3061]],\n       device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nfig, axes = plt.subplots(ncols=3, nrows=3, figsize=(12,8), dpi=150)\nfor i, ax in enumerate(axes.flat):\n    plot_idx = np.random.choice(np.arange(0, len(in_valid)))\n    true = np.concatenate([in_valid.numpy()[plot_idx,-1,:].reshape(-1), true_valid.numpy()[plot_idx,:].reshape(-1)])\n    pred = np.concatenate([in_valid.numpy()[plot_idx,-1,:].reshape(-1), pred_valid[plot_idx,:].reshape(-1)])\n    ax.plot(pred, color='red', label='preds')\n    ax.plot(true, color='green', label='true')\n    ax.vlines(trn_len-1, np.min(true), np.max(true), color='black')\n    if i == 0: ax.legend()\nfig.tight_layout();"
  },
  {
    "objectID": "monitosed_eoec.html",
    "href": "monitosed_eoec.html",
    "title": "Monitosed EOEC",
    "section": "",
    "text": "x,y  = preprocess_files_eoec(eeg_dir)\ndls = create_dls_eoec(x, y, bs)",
    "crumbs": [
      "tutorial",
      "Monitosed EOEC"
    ]
  },
  {
    "objectID": "monitosed_eoec.html#train-the-auto-encoder",
    "href": "monitosed_eoec.html#train-the-auto-encoder",
    "title": "Monitosed EOEC",
    "section": "Train the auto-encoder",
    "text": "Train the auto-encoder\n\nae_filename = ''\nacc_factor = 1\nmodel = stagerNetAAE(latent_dim=64, channels=x.shape[1], timestamps=x.shape[-1], acc_factor=acc_factor, dropout_rate=.3)\n\nmetrics = [rmse]\nlearn = Learner(dls, model, loss_func = model.ae_loss_func, metrics=metrics, opt_func=ranger)\nlearning_rate = learn.lr_find()\nprint('learning rate: '+str(learning_rate.valley))\nlearn.fit_flat_cos(n_epoch=100, lr=learning_rate.valley, \n                   cbs=[\n                        GradientAccumulation(n_acc=dls.bs*acc_factor),\n                        TrackerCallback(),\n                        SaveModelCallback(fname=ae_filename),\n                        EarlyStoppingCallback(min_delta=1e-4,patience=10)])\n\nstate_dict = torch.load(f'models/{ae_filename}.pth') # load the best weights",
    "crumbs": [
      "tutorial",
      "Monitosed EOEC"
    ]
  },
  {
    "objectID": "monitosed_eoec.html#train-the-adversarial-part",
    "href": "monitosed_eoec.html#train-the-adversarial-part",
    "title": "Monitosed EOEC",
    "section": "Train the Adversarial part",
    "text": "Train the Adversarial part\n\nmodel = stagerNetAAE(latent_dim=64, channels=x.shape[1], timestamps=x.shape[-1], acc_factor=acc_factor, adv_weight=0.9, recons_weight=0.1, dropout_rate=.3)\nmodel.load_state_dict(state_dict, strict=False)\nadv_filename = 'monitosed_pretrained_LEMON_aae_10s_EC_EO'\n\nmetrics = [LossAttrMetric(\"recons_loss\"),\n           LossAttrMetric(\"adv_loss\")]\nlearn = Learner(dls, model, loss_func=model.aae_loss_func_monitosed,\n               metrics=metrics, opt_func=ranger)\n\nlearn.fit_flat_cos(50, lr=2e-3,\n                        cbs=[\n                            GradientAccumulation(n_acc=dls.bs*acc_factor),\n                            TrackerCallback(monitor='valid_loss'),\n                            SaveModelCallback(fname=adv_filename, monitor='valid_loss'),\n                            EarlyStoppingCallback(min_delta=1e-4,patience=10,monitor='valid_loss'),\n                            UnfreezeFcCrit(switch_every=2)])\n\nstate_dict = torch.load(f'models/{adv_filename}.pth') # load the best weights\n\n\nsave_path = ''\n\nz, target = extract_latent(state_dict, save_path, save)\nplot_results(z.to(device),target.cpu(),filename=save_path)",
    "crumbs": [
      "tutorial",
      "Monitosed EOEC"
    ]
  },
  {
    "objectID": "monitosed_eoec.html#train-the-classifier-part",
    "href": "monitosed_eoec.html#train-the-classifier-part",
    "title": "Monitosed EOEC",
    "section": "Train the Classifier part",
    "text": "Train the Classifier part\n\nmodel = stagerNetAAE(latent_dim=64, channels=x.shape[1], timestamps=x.shape[-1], acc_factor=acc_factor, dropout_rate=.3, recons_weight=.1, adv_weight=.4, classif_weight=.5)\nmodel.load_state_dict(state_dict, strict=False)\nclassif_filename = 'monitosed_pretrained_LEMON_aae_classif_10s_EC_EO'\n\nmetrics = [accuracy, LossAttrMetric(\"recons_loss\"),\n           LossAttrMetric(\"adv_loss\"),\n           LossAttrMetric(\"classif_loss\")]\nlearn = Learner(dls, model, loss_func=model.aae_classif_loss_func_monitosed,\n               metrics=metrics, opt_func=ranger)\n\nlearn.fit_flat_cos(50, lr=1e-3,\n                        cbs=[\n                            GradientAccumulation(n_acc=dls.bs*acc_factor),\n                            TrackerCallback(monitor='valid_loss'),\n                            SaveModelCallback(fname=classif_filename, monitor='valid_loss'),\n                            EarlyStoppingCallback(min_delta=1e-4,patience=10,monitor='valid_loss'),\n                            UnfreezeFcCrit(switch_every=5)])\n\nstate_dict = torch.load(f'models/{classif_filename}.pth') # load the best weights\n\n\nsave_path = ''\n\nz, target = extract_latent(state_dict, save_path, save)\nplot_results(z.to(device),target.cpu(),filename=save_path)",
    "crumbs": [
      "tutorial",
      "Monitosed EOEC"
    ]
  },
  {
    "objectID": "data.visualization_old.html",
    "href": "data.visualization_old.html",
    "title": "data.visualization",
    "section": "",
    "text": "from monitosed.data.core import *\nfrom fastcore.xtras import Path\nimport mat73\n\n\npath_rest = Path(\"../_data/foot1/Rest\")\npath_vr = Path(\"../_data/foot1/VR\")\npath_rest.ls(), path_vr.ls()\n\n((#14) [Path('../_data/foot1/Rest/VR23_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR20_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR26_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR31_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR27_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR40_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR51_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR41_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR38_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR35_RestStim_data_clean.mat')...],\n (#15) [Path('../_data/foot1/VR/VR20_VRstim_data_clean.mat'),Path('../_data/foot1/VR/VR23_VRStim_data_clean.mat'),Path('../_data/foot1/VR/VR31_VRstim_data_clean.mat'),Path('../_data/foot1/VR/VR26_VRStim_data_clean.mat'),Path('../_data/foot1/VR/VR27_VRstim_data_clean.mat'),Path('../_data/foot1/VR/VR34_VRstim_data_clean.mat'),Path('../_data/foot1/VR/VR41_VRStim_data_clean.mat'),Path('../_data/foot1/VR/VR51_VRStim_data_clean.mat'),Path('../_data/foot1/VR/VR35_VRStim_data_clean.mat'),Path('../_data/foot1/VR/VR40_VRstim_data_clean.mat')...])\n\n\n\nmat_rest = read_data(mat73.loadmat(path_rest/'VR23_RestStim_data_clean.mat'))\nmat_vr = read_data(mat73.loadmat(path_vr/'VR23_VRStim_data_clean.mat'))\n\n\nplot_compare(mat_rest, mat_vr)\n\n                                                \n\n\n\nplot_3d(mat_rest)\n\n                                                \n\n\n\nplot_corr(mat_rest)\n\n\n\n\n\n\n\n\n\nplot_corr(mat_vr)"
  },
  {
    "objectID": "generation.cnn.transformer_ols.html",
    "href": "generation.cnn.transformer_ols.html",
    "title": "generation.cnn",
    "section": "",
    "text": "from monitosed.data.core import *\nfrom monitosed.models import *\nfrom monitosed.losses import *\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nimport mat73\nfrom fastcore.xtras import Path\nimport pandas as pd\nimport numpy as np\n\nfrom tsai.all import *"
  },
  {
    "objectID": "generation.cnn.transformer_ols.html#load-data",
    "href": "generation.cnn.transformer_ols.html#load-data",
    "title": "generation.cnn",
    "section": "Load data",
    "text": "Load data\n\nmats = load_mats(path)\n\n\n\n\nLoading: VR20_Reststim_data_clean.mat\nLoading: VR23_RestStim_data_clean.mat\nLoading: VR26_Reststim_data_clean.mat\nLoading: VR27_Reststim_data_clean.mat\nLoading: VR31_Reststim_data_clean.mat\nLoading: VR35_RestStim_data_clean.mat\nLoading: VR38_RestStim_data_clean.mat\nLoading: VR40_RestStim_data_clean.mat\nLoading: VR41_RestStim_data_clean.mat\nLoading: VR51_RestStim_data_clean.mat\nLoading: VR52_RestStim_data_clean.mat\nLoading: VR57_Reststim_data_clean.mat\nLoading: VR59_Reststim_data_clean.mat\nLoading: VR60_Reststim_data_clean.mat\n\n\n\n#Optional\nsignal_len = 500"
  },
  {
    "objectID": "generation.cnn.transformer_ols.html#get-labels",
    "href": "generation.cnn.transformer_ols.html#get-labels",
    "title": "generation.cnn",
    "section": "Get labels",
    "text": "Get labels\nThe first idea is to perform forecasting, i.e. predict the future signal. To do so, we have to cut the measured signal into a past and the future we would like to predict.\n/! the stimulus happens at 250\n\ntrn_len = 400\npred_len = signal_len-trn_len\nvalid_pct=0.2"
  },
  {
    "objectID": "generation.cnn.transformer_ols.html#datablock",
    "href": "generation.cnn.transformer_ols.html#datablock",
    "title": "generation.cnn",
    "section": "DataBlock",
    "text": "DataBlock\n\ngetters = [ItemGetter(0), ItemGetter(1)]\n\n\ndata = np.concatenate([read_data(mat) for mat in mats])\ndata = data[:,:,:signal_len]\ndata.shape\n\n(778, 173, 500)\n\n\n\nx = torch.from_numpy(np.concatenate(data, axis=0))[:,:trn_len].unsqueeze(1)\ny = torch.from_numpy(np.concatenate(data, axis=0))[:,trn_len:]\nx.shape, y.shape\n\n(torch.Size([134594, 1, 400]), torch.Size([134594, 100]))\n\n\n\nsplits = create_splits(mats)\n\n\n# For randperm\ntrain_ix = int((1-valid_pct)*len(mats))\nread_mats = [read_data(mat) for mat in mats]\nrearranged_mats = [new_rearrange(mat) for mat in read_mats]\nn_train_sample = np.concatenate(rearranged_mats[:train_ix]).shape[0]\nrandperm = torch.randperm(x.shape[0])\nrand_splits = (np.array(randperm[:n_train_sample]), np.array(randperm[n_train_sample:]))\n\n\nsplits[0].shape, splits[1].shape\n\n((103454,), (31140,))\n\n\n\ndls = get_ts_dls(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), splits=splits, bs=64)\n\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 1, 400]), torch.Size([64, 100]))\n\n\n\nplt.plot(xb[0][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nx = torch.tensor(x, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\n\n\nsplits\n\n(array([     0,      1,      2, ..., 103451, 103452, 103453]),\n array([103454, 103455, 103456, ..., 134591, 134592, 134593]))\n\n\n\nlearn = TSForecaster(x, y, splits=splits, arch=None, arch_config=dict(fc_dropout=.5), metrics=mae, bs=512, \n                         partial_n=.1, train_metrics=True)\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=3.0199516913853586e-05)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(50, 3e-5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nmae\ntime\n\n\n\n\n0\n25.173622\n3.608815\n00:05\n\n\n1\n24.929665\n3.563651\n00:05\n\n\n2\n24.462009\n3.522667\n00:05\n\n\n3\n25.698538\n3.431537\n00:05\n\n\n4\n23.974386\n3.352218\n00:05\n\n\n5\n22.580341\n3.258568\n00:05\n\n\n6\n21.513647\n3.211572\n00:05\n\n\n7\n20.432396\n3.140452\n00:05\n\n\n8\n19.476246\n3.053705\n00:05\n\n\n9\n18.601572\n2.997290\n00:05\n\n\n10\n17.794315\n2.936592\n00:05\n\n\n11\n17.291908\n2.936366\n00:05\n\n\n12\n18.159668\n2.944853\n00:05\n\n\n13\n17.328182\n2.885622\n00:05\n\n\n14\n18.016012\n2.899545\n00:05\n\n\n15\n17.231920\n2.864954\n00:05\n\n\n16\n17.686354\n2.876867\n00:05\n\n\n17\n16.886419\n2.839775\n00:05\n\n\n18\n16.312378\n2.815355\n00:05\n\n\n19\n16.000196\n2.830003\n00:05\n\n\n20\n15.509758\n2.770328\n00:05\n\n\n21\n16.271610\n2.778908\n00:05\n\n\n22\n15.689769\n2.758012\n00:05\n\n\n23\n15.463242\n2.777705\n00:05\n\n\n24\n15.091520\n2.754597\n00:05\n\n\n25\n14.782574\n2.734841\n00:05\n\n\n26\n14.576918\n2.732822\n00:05\n\n\n27\n14.468419\n2.734217\n00:05\n\n\n28\n15.538949\n2.714443\n00:05\n\n\n29\n15.023301\n2.723900\n00:05\n\n\n30\n14.637112\n2.710036\n00:05\n\n\n31\n14.417060\n2.700157\n00:05\n\n\n32\n14.222482\n2.694623\n00:05\n\n\n33\n14.037390\n2.688023\n00:05\n\n\n34\n13.806570\n2.656897\n00:05\n\n\n35\n13.799851\n2.684112\n00:05\n\n\n36\n13.664937\n2.660844\n00:05\n\n\n37\n14.657960\n2.695100\n00:05\n\n\n38\n14.342361\n2.685748\n00:05\n\n\n39\n14.004271\n2.658579\n00:05\n\n\n40\n13.771116\n2.651633\n00:05\n\n\n41\n13.672136\n2.661532\n00:05\n\n\n42\n13.592580\n2.661957\n00:05\n\n\n43\n13.488199\n2.655274\n00:05\n\n\n44\n13.515935\n2.667412\n00:05\n\n\n45\n13.554218\n2.679769\n00:05\n\n\n46\n13.466813\n2.651286\n00:05\n\n\n47\n14.413342\n2.682334\n00:05\n\n\n48\n14.081396\n2.662519\n00:05\n\n\n49\n13.804342\n2.643803\n00:05\n\n\n\n\n\n\nin_valid, pred_valid, true_valid = learn.get_preds(with_input=True)\n\n\n\n\n\n\n\n\n\nin_valid.shape\n\ntorch.Size([31140, 1, 400])\n\n\n\npred_valid.shape\n\ntorch.Size([31140, 100])\n\n\n\ntrue_valid.shape\n\ntorch.Size([31140, 100])\n\n\n\nfig, axes = plt.subplots(ncols=3, nrows=3, figsize=(12,8), dpi=150)\nfor i, ax in enumerate(axes.flat):\n    plot_idx = np.random.choice(np.arange(0, len(in_valid)))\n    true = np.concatenate([in_valid.numpy()[plot_idx,-1,:].reshape(-1), true_valid.numpy()[plot_idx,:].reshape(-1)])\n    pred = np.concatenate([in_valid.numpy()[plot_idx,-1,:].reshape(-1), pred_valid[plot_idx,:].reshape(-1)])\n    ax.plot(pred, color='red', label='preds')\n    ax.plot(true, color='green', label='true')\n    ax.vlines(trn_len-1, np.min(true), np.max(true), color='black')\n    if i == 0: ax.legend()\nfig.tight_layout();\n\n\n\n\n\n\n\n\n\nlearn.model\n\nInceptionTimePlus(\n  (backbone): Sequential(\n    (0): InceptionBlockPlus(\n      (inception): ModuleList(\n        (0): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): Conv1d(1, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n            )\n            (1): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n            )\n            (2): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): Conv1d(1, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (act): ReLU()\n        )\n        (1): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n            )\n            (1): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n            )\n            (2): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (act): ReLU()\n        )\n        (2): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n            )\n            (1): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n            )\n            (2): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (3): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n            )\n            (1): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n            )\n            (2): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (act): ReLU()\n        )\n        (4): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n            )\n            (1): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n            )\n            (2): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (act): ReLU()\n        )\n        (5): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n            )\n            (1): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n            )\n            (2): ConvBlock(\n              (0): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (shortcut): ModuleList(\n        (0): ConvBlock(\n          (0): Conv1d(1, 128, kernel_size=(1,), stride=(1,), bias=False)\n          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ModuleList(\n        (0): ReLU()\n        (1): ReLU()\n      )\n      (add): Add\n    )\n  )\n  (head): Sequential(\n    (0): create_lin_nd_head(\n      (0): fastai.layers.Flatten(full=False)\n      (1): Linear(in_features=51200, out_features=100, bias=True)\n      (2): Reshape(bs, 100)\n    )\n  )\n)\n\n\n\ndata.shape\n\n(778, 173, 500)\n\n\n\nX = torch.from_numpy(np.concatenate(data, axis=0))[:,:400].squeeze(1)\n\n\nX = torch.tensor(x, dtype=torch.float32)\n\n\nX.shape\n\ntorch.Size([134594, 1, 400])\n\n\n\nx.shape\n\ntorch.Size([134594, 1, 400])\n\n\n\nSlidingWindowSplitter??\n\n\nX, y = SlidingWindowSplitter(30, horizon=1)(X)\n\nMemoryError: Unable to allocate 361. GiB for an array with shape (134534, 30, 400, 30) and data type float64\n\n\n\nplt.plot(X[0][0])\n\n\n\n\n\n\n\n\n\nx.squeeze().shape\n\ntorch.Size([134594, 400])\n\n\n\nplt.plot(x.squeeze()[0])\n\n\n\n\n\n\n\n\n\ny.shape\n\n(134564, 300)\n\n\n\nTSSplitter??\n\n\nspl = (splits[0], splits[1][:-30])\n\n\nspl\n\n(array([     0,      1,      2, ..., 103451, 103452, 103453]),\n array([103454, 103455, 103456, ..., 134561, 134562, 134563]))\n\n\n\nsplits = create_splits(mats)\n\n\nx = torch.tensor(x, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\n\n\nTSForecaster??\n\n\n#splits = TSSplitter()(y)\nbatch_tfms = [TSStandardize(by_var=True)]\nlearn = TSForecaster(x, y, splits=spl, batch_tfms=batch_tfms, arch=None, arch_config=dict(fc_dropout=.5), metrics=mae, bs=128, \n                         partial_n=.1, train_metrics=True)\n\n\nlearn.fit_one_cycle(30)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nmae\ntime\n\n\n\n\n0\n18.868866\n3.218201\n00:06\n\n\n1\n17.290142\n2.993946\n00:06\n\n\n2\n17.108524\n2.998340\n00:06\n\n\n3\n16.940899\n2.950398\n00:06\n\n\n4\n16.245564\n2.933988\n00:06\n\n\n5\n16.070183\n2.921601\n00:06\n\n\n6\n16.117336\n2.902923\n00:06\n\n\n7\n15.815660\n2.882623\n00:06\n\n\n8\n15.134312\n2.804292\n00:06\n\n\n9\n14.659487\n2.765582\n00:06\n\n\n10\n14.108326\n2.718621\n00:06\n\n\n11\n13.809961\n2.701286\n00:06\n\n\n12\n13.291049\n2.643216\n00:06\n\n\n13\n13.407202\n2.658267\n00:06\n\n\n14\n12.563900\n2.562838\n00:06\n\n\n15\n11.976115\n2.514545\n00:06\n\n\n16\n11.747484\n2.481740\n00:06\n\n\n17\n11.608392\n2.449190\n00:06\n\n\n18\n11.826988\n2.468491\n00:06\n\n\n19\n10.952284\n2.387222\n00:06\n\n\n20\n10.306363\n2.339887\n00:06\n\n\n21\n10.048980\n2.302203\n00:06\n\n\n22\n9.634402\n2.275650\n00:06\n\n\n23\n9.687000\n2.271304\n00:06\n\n\n24\n9.592413\n2.249695\n00:06\n\n\n25\n9.383493\n2.211651\n00:06\n\n\n26\n9.183927\n2.192322\n00:06\n\n\n27\n9.052883\n2.185113\n00:06\n\n\n28\n8.993866\n2.178875\n00:06\n\n\n29\n8.986015\n2.170226\n00:06\n\n\n\n\n\n\nin_valid, pred_valid, true_valid = learn.get_preds(with_input=True)\n\n\n\n\n\n\n\n\n\nin_valid.shape\n\ntorch.Size([31110, 1, 400])\n\n\n\npred_valid.shape\n\ntorch.Size([31110, 100])\n\n\n\ntrue_valid.shape\n\ntorch.Size([31110, 100])\n\n\n\nfig, axes = plt.subplots(ncols=3, nrows=3, figsize=(12,8), dpi=150)\nfor i, ax in enumerate(axes.flat):\n    plot_idx = np.random.choice(np.arange(0, len(in_valid)))\n    true = np.concatenate([in_valid.numpy()[plot_idx,-1,:].reshape(-1), true_valid.numpy()[plot_idx,:].reshape(-1)])\n    pred = np.concatenate([in_valid.numpy()[plot_idx,-1,:].reshape(-1), pred_valid[plot_idx,:].reshape(-1)])\n    ax.plot(pred, color='red', label='preds')\n    ax.plot(true, color='green', label='true')\n    ax.vlines(trn_len-1, np.min(true), np.max(true), color='black')\n    if i == 0: ax.legend()\nfig.tight_layout();\n\n\n\n\n\n\n\n\n\nts = get_forecasting_time_series('Sunspots')\nif ts is not None: # This is to prevent a test fail when the data server is not available\n    X, y = SlidingWindowSplitter(60, horizon=1)(ts)\n    splits = TSSplitter(235)(y)\n    batch_tfms = [TSStandardize(by_var=True)]\n    learn = TSForecaster(X, y, splits=splits, batch_tfms=batch_tfms, arch=None, arch_config=dict(fc_dropout=.5), metrics=mae, bs=512, \n                         partial_n=.1, train_metrics=True)\n    #learn.fit_one_cycle(1)\n\nDataset: Sunspots\ndownloading data...\n...done. Path = data/forecasting/Sunspots.csv\n\n\n\n\n\n\n\n\n\nTypeError: expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=double, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))"
  },
  {
    "objectID": "data.core_old.html",
    "href": "data.core_old.html",
    "title": "data.core",
    "section": "",
    "text": "path = Path(\"../_data/foot1/Rest\"); path.ls()\n\n(#14) [Path('../_data/foot1/Rest/VR23_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR20_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR26_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR31_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR27_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR40_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR51_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR41_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR38_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR35_RestStim_data_clean.mat')...]\n\n\n\nmats = load_mats(path, 3)\n\n\n\n\nLoading: VR23_RestStim_data_clean.mat\nLoading: VR20_Reststim_data_clean.mat\nLoading: VR26_Reststim_data_clean.mat\n\n\n\nread_data(mats[0])\n\narray([[[ -1.8796305 ,  -1.57882798,  -1.22401914, ...,  -0.90901878,\n          -0.95998329,  -0.98888413],\n        [ -1.48295197,  -1.40576813,  -1.39516744, ...,  -0.26170848,\n          -0.08653054,   0.04954708],\n        [  3.74463354,   2.28465188,   0.49199618, ...,   1.96247153,\n           1.47387488,   0.90841343],\n        ...,\n        [ -3.66535423,  -2.82230685,  -1.82522995, ...,   1.12795429,\n           0.90016588,   0.26821923],\n        [ -5.27174769,  -4.13237082,  -2.76992229, ...,   1.02706457,\n          -0.1265582 ,  -1.5465144 ],\n        [ -5.04579727,  -4.94524743,  -4.80847541, ...,  -0.85468755,\n          -1.39623986,  -2.02535793]],\n\n       [[  0.25069887,   0.27084653,   0.2341677 , ...,   4.52802476,\n           3.47379513,   2.49300888],\n        [  3.46406361,   3.42280404,   3.29053777, ...,   4.48878193,\n           3.39170582,   2.30179667],\n        [  5.05976625,   4.83057146,   4.54744324, ...,   6.29398407,\n           4.92086394,   3.37701366],\n        ...,\n        [  6.50958796,   7.03879954,   7.26554737, ...,   2.83744567,\n           1.90820615,   0.85470884],\n        [  6.87457167,   6.56798579,   6.20699458, ...,   3.89821873,\n           3.60998201,   3.08282046],\n        [  3.19691682,   3.38618972,   3.44493285, ...,  -0.84186665,\n          -0.31637178,   0.09611819]],\n\n       [[ -0.47161505,  -0.14728712,   0.17522135, ...,  -1.75078123,\n          -2.35375691,  -3.10984125],\n        [  3.65661719,   4.29718649,   4.89617149, ...,  -2.94138062,\n          -3.54560916,  -4.13521478],\n        [  6.33409114,   6.42362401,   6.50679535, ...,  -7.74656399,\n          -9.39941956, -10.76897988],\n        ...,\n        [  4.44619269,   4.6431496 ,   4.64524656, ...,  -1.81260874,\n          -2.01358111,  -2.41375389],\n        [  4.27170927,   3.97836085,   3.75096553, ...,  -3.83770585,\n          -4.69461887,  -5.3680895 ],\n        [  3.29707127,   3.13546668,   2.88709392, ...,   4.57024327,\n           4.70019215,   4.60659858]],\n\n       ...,\n\n       [[ -4.87133216,  -4.20087099,  -3.51749525, ...,   5.63894965,\n           5.57783056,   5.44757343],\n        [ -0.2900357 ,   0.18093022,   0.55828147, ...,   0.84942795,\n           0.73367026,   0.60773095],\n        [ -3.19244185,  -2.39573948,  -1.68394943, ...,  -6.96001147,\n          -6.5864767 ,  -5.67958471],\n        ...,\n        [ -0.05297898,  -1.53451736,  -2.94645353, ...,  -4.11086175,\n          -4.37665331,  -4.388515  ],\n        [  3.14019978,   2.25948692,   1.16756733, ...,  -3.52952333,\n          -3.04952133,  -2.37597292],\n        [  0.74025721,   0.70875447,   0.77081654, ...,  -5.25810428,\n          -4.57361571,  -3.86454608]],\n\n       [[ -2.66071795,  -2.16685149,  -1.73570276, ...,   2.32753501,\n           1.87292161,   1.27207588],\n        [ -5.82329124,  -4.77143232,  -3.58726718, ...,   2.64951668,\n           1.67217452,   0.42768568],\n        [-10.48608913,  -8.95927433,  -7.25830004, ...,   9.45582631,\n           6.80956221,   3.79829877],\n        ...,\n        [ -8.36838473,  -7.67721764,  -6.92373593, ...,   5.33446067,\n           5.04973085,   4.34107238],\n        [-11.08040253, -10.07404281,  -9.00438068, ...,   5.31748203,\n           5.05945855,   4.45491416],\n        [ -3.27777159,  -2.87376875,  -2.39303818, ...,   7.22775408,\n           8.15789986,   9.01792071]],\n\n       [[ -0.29623206,  -0.41829409,  -0.62070483, ...,  -2.09183234,\n          -1.25441148,  -0.29340329],\n        [ -2.64292679,  -2.67426407,  -2.63700486, ...,   0.06919099,\n           0.15791898,   0.14993883],\n        [ -6.01049861,  -5.72280978,  -5.22158114, ...,   9.27238609,\n           9.5695711 ,   9.05179122],\n        ...,\n        [ -2.66768282,  -2.76676345,  -2.85403927, ...,  -1.07428595,\n          -0.09754323,   0.84844932],\n        [ -0.56612808,  -1.70416205,  -2.76654541, ...,  -2.00427761,\n          -2.36319929,  -2.60414378],\n        [  0.99683259,   0.65833372,   0.27304946, ...,   1.57625413,\n           1.52992107,   1.42589499]]])\n\n\n\nprepare_train_data(mats)\n\narray([[[ -1.8796305 ,  -1.57882798,  -1.22401914, ...,  -0.90901878,\n          -0.95998329,  -0.98888413],\n        [ -1.48295197,  -1.40576813,  -1.39516744, ...,  -0.26170848,\n          -0.08653054,   0.04954708],\n        [  3.74463354,   2.28465188,   0.49199618, ...,   1.96247153,\n           1.47387488,   0.90841343],\n        ...,\n        [ -3.66535423,  -2.82230685,  -1.82522995, ...,   1.12795429,\n           0.90016588,   0.26821923],\n        [ -5.27174769,  -4.13237082,  -2.76992229, ...,   1.02706457,\n          -0.1265582 ,  -1.5465144 ],\n        [ -5.04579727,  -4.94524743,  -4.80847541, ...,  -0.85468755,\n          -1.39623986,  -2.02535793]],\n\n       [[  0.25069887,   0.27084653,   0.2341677 , ...,   4.52802476,\n           3.47379513,   2.49300888],\n        [  3.46406361,   3.42280404,   3.29053777, ...,   4.48878193,\n           3.39170582,   2.30179667],\n        [  5.05976625,   4.83057146,   4.54744324, ...,   6.29398407,\n           4.92086394,   3.37701366],\n        ...,\n        [  6.50958796,   7.03879954,   7.26554737, ...,   2.83744567,\n           1.90820615,   0.85470884],\n        [  6.87457167,   6.56798579,   6.20699458, ...,   3.89821873,\n           3.60998201,   3.08282046],\n        [  3.19691682,   3.38618972,   3.44493285, ...,  -0.84186665,\n          -0.31637178,   0.09611819]],\n\n       [[ -0.47161505,  -0.14728712,   0.17522135, ...,  -1.75078123,\n          -2.35375691,  -3.10984125],\n        [  3.65661719,   4.29718649,   4.89617149, ...,  -2.94138062,\n          -3.54560916,  -4.13521478],\n        [  6.33409114,   6.42362401,   6.50679535, ...,  -7.74656399,\n          -9.39941956, -10.76897988],\n        ...,\n        [  4.44619269,   4.6431496 ,   4.64524656, ...,  -1.81260874,\n          -2.01358111,  -2.41375389],\n        [  4.27170927,   3.97836085,   3.75096553, ...,  -3.83770585,\n          -4.69461887,  -5.3680895 ],\n        [  3.29707127,   3.13546668,   2.88709392, ...,   4.57024327,\n           4.70019215,   4.60659858]],\n\n       ...,\n\n       [[  1.50720315,   1.56975169,   1.59000298, ...,   2.50699839,\n           2.23776733,   1.99485749],\n        [  0.09255252,  -0.31866285,  -0.64755657, ...,   1.31884983,\n           1.26329342,   1.26151879],\n        [  1.69057533,   1.46894744,   1.32789718, ...,  -0.89534506,\n          -1.0703412 ,  -1.20661143],\n        ...,\n        [  4.31374144,   3.98289932,   3.66469825, ...,  -1.64673732,\n          -1.72455977,  -1.81201622],\n        [  2.18833201,   1.87316145,   1.60585909, ...,  -1.80201163,\n          -1.8873458 ,  -1.94344961],\n        [  1.67915609,   1.2521313 ,   0.81808252, ...,  -2.9404571 ,\n          -2.87559828,  -2.80442198]],\n\n       [[ -1.77682936,  -2.14049287,  -2.44876869, ...,  -3.01424678,\n          -2.98142234,  -3.22388455],\n        [-10.52369248, -10.54350152, -10.4418317 , ...,   1.60826583,\n           2.28535708,   2.71627433],\n        [ -5.18367196,  -5.44202817,  -5.54122161, ...,   3.64129643,\n           3.85338825,   3.90718836],\n        ...,\n        [ -2.59934081,  -2.697829  ,  -2.68705069, ...,   4.49526146,\n           4.95455708,   5.26769594],\n        [ -1.37949436,  -1.47495599,  -1.48649458, ...,   1.61518345,\n           1.86582402,   1.9653936 ],\n        [  3.76893988,   3.93722399,   3.99929963, ...,   0.82547948,\n           1.12138224,   1.53255226]],\n\n       [[  4.63199157,   4.2004806 ,   3.62956549, ...,   1.32587381,\n           0.71340269,   0.17478743],\n        [  9.47581089,   9.01413907,   8.41359603, ...,  -2.11381454,\n          -2.0548406 ,  -1.88117203],\n        [  5.73896832,   4.93463191,   4.06797993, ...,  -0.59227381,\n          -0.3488382 ,  -0.1202621 ],\n        ...,\n        [  3.94300728,   2.9942528 ,   1.9297952 , ...,  -2.53728615,\n          -2.36290858,  -2.21914269],\n        [  6.22715379,   5.56379764,   4.76265561, ...,   0.96217077,\n           1.32341895,   1.62603072],\n        [ -1.39443659,  -0.95981367,  -0.46403909, ...,  -0.37266013,\n          -0.1248667 ,   0.06978394]]])"
  },
  {
    "objectID": "generation.cnn-unet_old.html",
    "href": "generation.cnn-unet_old.html",
    "title": "generation.cnn",
    "section": "",
    "text": "from monitosed.data.core import *\nfrom monitosed.models import *\nfrom monitosed.losses import *\nimport mat73\nfrom fastcore.xtras import Path\nimport pandas as pd\nimport numpy as np\n\nfrom tsai.all import *"
  },
  {
    "objectID": "generation.cnn-unet_old.html#load-data",
    "href": "generation.cnn-unet_old.html#load-data",
    "title": "generation.cnn",
    "section": "Load data",
    "text": "Load data\n\nmats = load_mats(path)\n\n\n\n\nLoading: VR20_Reststim_data_clean.mat\nLoading: VR23_RestStim_data_clean.mat\nLoading: VR26_Reststim_data_clean.mat\nLoading: VR27_Reststim_data_clean.mat\nLoading: VR31_Reststim_data_clean.mat\nLoading: VR35_RestStim_data_clean.mat\nLoading: VR38_RestStim_data_clean.mat\nLoading: VR40_RestStim_data_clean.mat\nLoading: VR41_RestStim_data_clean.mat\nLoading: VR51_RestStim_data_clean.mat\nLoading: VR52_RestStim_data_clean.mat\nLoading: VR57_Reststim_data_clean.mat\nLoading: VR59_Reststim_data_clean.mat\nLoading: VR60_Reststim_data_clean.mat\n\n\n\n#Optional\nsignal_len = 500"
  },
  {
    "objectID": "generation.cnn-unet_old.html#get-labels",
    "href": "generation.cnn-unet_old.html#get-labels",
    "title": "generation.cnn",
    "section": "Get labels",
    "text": "Get labels\nThe first idea is to perform forecasting, i.e. predict the future signal. To do so, we have to cut the measured signal into a past and the future we would like to predict.\n/! the stimulus happens at 250\n\ntrn_len = 300\npred_len = signal_len-trn_len\nvalid_pct=0.2"
  },
  {
    "objectID": "generation.cnn-unet_old.html#datablock",
    "href": "generation.cnn-unet_old.html#datablock",
    "title": "generation.cnn",
    "section": "DataBlock",
    "text": "DataBlock\n\ndata = np.concatenate([read_data(mat) for mat in mats])\ndata = data[:,:,:signal_len]\ndata.shape\n\n(778, 173, 500)\n\n\n\ndata.shape\n\n(778, 173, 500)\n\n\n\n#x = torch.from_numpy(data)[:,:, :trn_len]\nx = torch.from_numpy(data.copy())\n#y = torch.from_numpy(data)[:,:, trn_len:]\ny = torch.from_numpy(data.copy())\nx.shape, y.shape\n\n(torch.Size([778, 173, 500]), torch.Size([778, 173, 500]))\n\n\n\nx[:,:,trn_len:] = torch.randn(*x[:,:,trn_len:].shape)\n\n\ngetters = [ItemGetter(0), ItemGetter(1)]\n\n\ntrain_ix = int((1-valid_pct)*len(x))\n\n\nsplits = (np.arange(0, train_ix), np.arange(train_ix, len(x)))\n\n\nx = torch.tensor(x, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\n\n\ndblock = DataBlock(blocks=(TSTensorBlock, TSTensorBlock),\n                   getters=getters,\n                   splitter=IndexSplitter(splits[1]),\n                   item_tfms=None,\n                   batch_tfms=None)\n\nsource = itemify(x,y)\n\ndls = dblock.dataloaders(source, bs=4, val_bs=8, num_workers=8)\n\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([4, 173, 500]), torch.Size([4, 173, 500]))\n\n\n\nplt.plot(xb[0][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(xb[0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(yb[0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nnet = StagerUNet_M(173, embed_dim=1)\n\n\nxb.shape\n\ntorch.Size([4, 173, 500])\n\n\n\nxout = net(xb.to('cpu'))\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(xout[0].to('cpu').detach().numpy())\n\n\n\n\n\n\n\n\n\nclass CustomLoss(nn.Module):\n    def __init__(self, size_average=None, reduce=None, alpha=0.5, reduction: str = 'mean') -&gt; None:\n        super(CustomLoss, self).__init__()\n        store_attr()\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        l1 = F.l1_loss(input, target, reduction=self.reduction)\n        mse = F.mse_loss(input, target, reduction=self.reduction)\n        return self.alpha*l1 + (1-self.alpha)*mse\n\n\nlearn = Learner(dls, net, loss_func=CustomLoss(), metrics=[mae, rmse], wd=1, cbs=ShowGraph())\n\n\nlearn.fit_one_cycle(50, 1e-1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae\n_rmse\ntime\n\n\n\n\n0\n8.683238\n7.330503\n2.198819\n3.530182\n00:08\n\n\n1\n8.341117\n7.506594\n2.291394\n3.566762\n00:07\n\n\n2\n7.650790\n7.379282\n2.248701\n3.536928\n00:07\n\n\n3\n8.414534\n7.686992\n2.323935\n3.612485\n00:07\n\n\n4\n10.199923\n8.658038\n2.567312\n3.840412\n00:07\n\n\n5\n9.465446\n8.778734\n2.634310\n3.863050\n00:07\n\n\n6\n9.626359\n9.003091\n2.616670\n3.922947\n00:07\n\n\n7\n9.537219\n9.161760\n2.654912\n3.958359\n00:07\n\n\n8\n10.437703\n9.203692\n2.692430\n3.964209\n00:07\n\n\n9\n10.932980\n9.572007\n2.781928\n4.045007\n00:07\n\n\n10\n10.355486\n9.179281\n2.667349\n3.961214\n00:07\n\n\n11\n10.432359\n8.885451\n2.584333\n3.896995\n00:07\n\n\n12\n10.179681\n8.932239\n2.616106\n3.904916\n00:07\n\n\n13\n11.126889\n8.931714\n2.609179\n3.905669\n00:07\n\n\n14\n10.669036\n8.930193\n2.629878\n3.902628\n00:07\n\n\n15\n12.035663\n13.616746\n3.597637\n4.861672\n00:07\n\n\n16\n10.201404\n9.093315\n2.675924\n3.938363\n00:07\n\n\n17\n10.222593\n9.100693\n2.659525\n3.942317\n00:07\n\n\n18\n11.516212\n9.075278\n2.667088\n3.934904\n00:08\n\n\n19\n10.148969\n8.971939\n2.658292\n3.909679\n00:07\n\n\n20\n10.331649\n8.607456\n2.573012\n3.826473\n00:07\n\n\n21\n10.798733\n9.618244\n2.809788\n4.052986\n00:07\n\n\n22\n10.140813\n8.693885\n2.557110\n3.851059\n00:07\n\n\n23\n10.510763\n9.935025\n2.841624\n4.126552\n00:07\n\n\n24\n9.753046\n8.207175\n2.435619\n3.738814\n00:08\n\n\n25\n9.223177\n8.761350\n2.576370\n3.866048\n00:07\n\n\n26\n10.025462\n9.280467\n2.783665\n3.972061\n00:08\n\n\n27\n9.474289\n8.305398\n2.467414\n3.760769\n00:07\n\n\n28\n9.725287\n9.475865\n2.757726\n4.024178\n00:08\n\n\n29\n9.189100\n7.895681\n2.346644\n3.666704\n00:07\n\n\n30\n9.656518\n8.121314\n2.429764\n3.716566\n00:08\n\n\n31\n9.059213\n8.177448\n2.421543\n3.732741\n00:08\n\n\n32\n9.228088\n7.932686\n2.391436\n3.670686\n00:08\n\n\n33\n8.996842\n7.988217\n2.388706\n3.686154\n00:08\n\n\n34\n10.009800\n7.491091\n2.285060\n3.563302\n00:08\n\n\n35\n9.154017\n7.931686\n2.364927\n3.674023\n00:07\n\n\n36\n9.580307\n8.622842\n2.612033\n3.825396\n00:08\n\n\n37\n8.347675\n7.697505\n2.330055\n3.614548\n00:08\n\n\n38\n8.934550\n7.459208\n2.240484\n3.560608\n00:08\n\n\n39\n8.771791\n7.361359\n2.252929\n3.531259\n00:08\n\n\n40\n7.997032\n7.068879\n2.151896\n3.462060\n00:08\n\n\n41\n7.936912\n7.018462\n2.126862\n3.451096\n00:08\n\n\n42\n8.056854\n7.143943\n2.184906\n3.478934\n00:08\n\n\n43\n7.695261\n6.776055\n2.051615\n3.391238\n00:08\n\n\n44\n7.575338\n6.717569\n2.041689\n3.375418\n00:08\n\n\n45\n7.308348\n6.494400\n1.984564\n3.317263\n00:08\n\n\n46\n6.936240\n6.352727\n1.909878\n3.285662\n00:08\n\n\n47\n6.328500\n6.174939\n1.849013\n3.240504\n00:08\n\n\n48\n5.991862\n6.096416\n1.816103\n3.221293\n00:08\n\n\n49\n5.966402\n6.065446\n1.805477\n3.213318\n00:08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nin_valid, pred_valid, true_valid = learn.get_preds(with_input=True)\n\n\n\n\n\n\n\n\n\nin_valid.shape\n\ntorch.Size([156, 173, 500])\n\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(in_valid[1].to('cpu').detach().numpy())\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(true_valid[1].to('cpu').detach().numpy())\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(pred_valid[1].to('cpu').detach().numpy())\n\n\n\n\n\n\n\n\n\ntrn_len = 250\npred_len = signal_len-trn_len\nvalid_pct=0.2\n\n\ndata = np.concatenate([read_data(mat) for mat in mats])\ndata = data[:,:,:signal_len]\ndata.shape\n\n(778, 173, 500)\n\n\n\ndata.shape\n\n(778, 173, 500)\n\n\n\nx = torch.from_numpy(data.copy())[:,:, :trn_len]\ny = torch.from_numpy(data.copy())[:,:, trn_len:]\nx.shape, y.shape\n\n(torch.Size([778, 173, 250]), torch.Size([778, 173, 250]))\n\n\n\n#x[:,:,trn_len:] = torch.randn(*x[:,:,trn_len:].shape)\n\n\ngetters = [ItemGetter(0), ItemGetter(1)]\n\n\ntrain_ix = int((1-valid_pct)*len(x))\n\n\nsplits = (np.arange(0, train_ix), np.arange(train_ix, len(x)))\n\n\nx = torch.tensor(x, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\n\n\ndblock = DataBlock(blocks=(TSTensorBlock, TSTensorBlock),\n                   getters=getters,\n                   splitter=IndexSplitter(splits[1]),\n                   item_tfms=None,\n                   batch_tfms=None)\n\nsource = itemify(x,y)\n\ndls = dblock.dataloaders(source, bs=4, val_bs=8, num_workers=8)\n\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([4, 173, 250]), torch.Size([4, 173, 250]))\n\n\n\nplt.plot(xb[0][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(xb[0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(yb[0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nclass StagerUNet_M(nn.Module):\n    def __init__(self, channels, dropout_rate=0.5, embed_dim=100, print_shape=False):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, channels, (channels, 1), stride=(1, 1), padding=(0, 0))\n        self.conv2 = nn.Conv2d(1, 16, (1, 51), stride=(1, 1), padding=(0, 25))\n        self.conv3 = nn.Conv2d(16, 16, (1, 51), stride=(1, 1), padding=(0, 25))\n        self.conv4 = nn.Conv2d(16, 16, (1, 51), stride=(1, 1), padding=(0, 25))\n\n        self.dropout_rate = dropout_rate\n        self.embed_dim = embed_dim\n        \n        self.upconv4 = nn.Conv2d(16, 16, (1, 51), stride=(1, 1), padding=(0, 25))\n        self.upconv3 = nn.Conv2d(16, 16, (1, 51), stride=(1, 1), padding=(0, 25))\n        self.upconv2 = nn.Conv2d(16, 1, (1, 51), stride=(1, 1), padding=(0, 25))\n        self.upconv1 = nn.Conv2d(1, channels, (channels, 1), stride=(1, 1), padding=(0, 0))\n        self.upconv0 = nn.Conv2d(1, 1, (3, 5), stride=(1, 1), padding=(1, 2))\n\n        self.batchnorm1 = nn.BatchNorm2d(16)\n        self.batchnorm2 = nn.BatchNorm2d(16)\n        self.batchnorm3 = nn.BatchNorm2d(16)\n        \n        self.upbatchnorm1 = nn.BatchNorm2d(16)\n        self.upbatchnorm2 = nn.BatchNorm2d(16)\n        self.upbatchnorm3 = nn.BatchNorm2d(16)\n        \n        self.print_shape = print_shape\n\n    \n    def forward(self, x):\n\n        x = torch.unsqueeze(x, 1)\n        \n        if self.print_shape: print(x.shape)\n\n\n        x = self.conv1(x)\n        if self.print_shape: print(x.shape)\n\n        x = x.permute(0, 2, 1, 3)\n        if self.print_shape: print(x.shape)\n        x = self.conv2(x)\n        \n        if self.print_shape: print(x.shape)\n        x, ix1 = F.max_pool2d_with_indices(x, (1,5))\n        x = F.relu(x)\n        if self.print_shape: print(x.shape)\n        x = self.batchnorm1(x)\n        \n        x = self.conv3(x)\n        \n        if self.print_shape: print(x.shape)\n        x, ix2 = F.max_pool2d_with_indices(x, (1,5))\n        x = F.relu(x)                               \n        x = self.batchnorm2(x)     \n        \n        x = self.conv4(x)\n        \n        if self.print_shape: print(x.shape)\n        x, ix3 = F.max_pool2d_with_indices(x, (1,5))\n        x = F.relu(x)                               \n        x = self.batchnorm3(x) \n        \n        \n        if self.print_shape: print(x.shape)\n        x = F.relu(F.max_unpool2d(x, ix3, (1,5)))   \n        x = self.upbatchnorm3(x)   \n        if self.print_shape: print(x.shape)\n        x = self.upconv4(x) \n        \n        \n        if self.print_shape: print(x.shape)\n        x = F.relu(F.max_unpool2d(x, ix2, (1,5)))   \n        x = self.upbatchnorm2(x)   \n        if self.print_shape: print(x.shape)\n        \n        x = self.upconv3(x) \n        \n        \n        if self.print_shape: print(x.shape)\n        \n        x = F.relu(F.max_unpool2d(x, ix1, (1,5)))\n        x = self.upbatchnorm1(x) \n        \n        x = self.upconv2(x) \n        if self.print_shape: print(x.shape)\n        \n        \n        x = self.upconv1(x)\n        \n        #x = x.permute(0, 2, 1, 3)\n        if self.print_shape: print(x.shape) \n            \n        #x = self.upconv0(x)\n        \n        x = torch.squeeze(x, 2)    \n        \n        if self.print_shape: print(x.shape) \n    \n        \n        return x\n\n\nnet = StagerUNet_M(173, embed_dim=1, print_shape=False)\n\n\nxout = net(xb.to('cpu'))\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(xout[0].to('cpu').detach().numpy())\n\n\n\n\n\n\n\n\n\nlearn = Learner(dls, net, loss_func=CustomLoss(), metrics=[mae, rmse], wd=1, cbs=ShowGraph())\n\n\nlearn.fit_one_cycle(10, 1e-1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae\n_rmse\ntime\n\n\n\n\n0\n14.659051\n14.726736\n3.593305\n5.085289\n00:05\n\n\n1\n214.715561\n14.947932\n3.607458\n5.127222\n00:04\n\n\n2\n24.290987\n14.891907\n3.595215\n5.117479\n00:05\n\n\n3\n16.106245\n14.899280\n3.596800\n5.118765\n00:05\n\n\n4\n15.731718\n14.595378\n3.551017\n5.063570\n00:04\n\n\n5\n15.034883\n13.763484\n3.423404\n4.909538\n00:05\n\n\n6\n15.084778\n14.826571\n3.591049\n5.105104\n00:05\n\n\n7\n14.762893\n14.239727\n3.496456\n4.998300\n00:05\n\n\n8\n14.111123\n13.907266\n3.460627\n4.934968\n00:05\n\n\n9\n13.942387\n13.868322\n3.448653\n4.928285\n00:05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nin_valid, pred_valid, true_valid = learn.get_preds(with_input=True)\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(in_valid[1].to('cpu').detach().numpy())\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(true_valid[1].to('cpu').detach().numpy())\n\n\nplt.figure(figsize=(12,6))\nplt.imshow(pred_valid[1].to('cpu').detach().numpy())"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "LossAttrMetric\n\n LossAttrMetric (attr)\n\nBlueprint for defining a metric\n\n\n\nUnfreezeFcCrit\n\n UnfreezeFcCrit (switch_every:int=7)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\n\n\nGetLatentSpace\n\n GetLatentSpace (cycle_len=None)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\n\n\nget_lds_kernel_window\n\n get_lds_kernel_window (kernel, ks, sigma)\n\n\n\n\nget_normalized_scores\n\n get_normalized_scores (display=False)\n\nCompute the regularized linear regression of the latent space wrt the labels\n\n\n\ndistrib_regul_regression\n\n distrib_regul_regression (z, target, nbins:int=100, get_reg:bool=False)\n\nCreate an histogram-like plot of the mean target value by x-bins\n\n\n\nhist_lab\n\n hist_lab (preds, target, nbins=42, reg=True)\n\n\n\n\nplot_results\n\n plot_results (z, lab_gather, filename, nbins=24)",
    "crumbs": [
      "utils",
      "utils"
    ]
  },
  {
    "objectID": "regression.cnn_old.html",
    "href": "regression.cnn_old.html",
    "title": "classification.cnn",
    "section": "",
    "text": "from monitosed.classification.core import *\nfrom monitosed.data.core import *\n\nfrom fastcore.xtras import Path\nfrom tsai.all import *\n\n\nusers, labels = get_users_labels('../_data/Smarthy2_Behavioral.xlsx', drop_ixs=[0,2,7, 12, 13, 16])\n\n\npath = Path(\"../_data/foot1/Rest\"); path.ls()\n\n(#14) [Path('../_data/foot1/Rest/VR23_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR20_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR26_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR31_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR27_Reststim_data_clean.mat'),Path('../_data/foot1/Rest/VR40_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR51_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR41_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR38_RestStim_data_clean.mat'),Path('../_data/foot1/Rest/VR35_RestStim_data_clean.mat')...]\n\n\n\nmats = load_mats(path)\n\n\n\n\nLoading: VR23_RestStim_data_clean.mat\nLoading: VR20_Reststim_data_clean.mat\n\n\n\n#Optional\nsignal_len = 1500\n\n\nx = stack_trials(mats, signal_len)\ny = prepare_labels(mats, labels.values)\nx.shape, y.shape\n\n(torch.Size([19376, 1, 1500]), torch.Size([19376]))\n\n\n\nread_mats = [read_data(mat) for mat in mats]\nrearranged_mats = [new_rearrange(mat) for mat in read_mats]\n\n\nsplits = create_splits(mats)\n\n\ntfms  = [None, [TSRegression()]]\nbatch_tfms = TSStandardize(by_sample=True, by_var=True)\n#dls = get_ts_dls(x, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=128)\ndls = get_ts_dls(x, y, splits=splits, tfms=tfms, bs=128)\n\n\nxb,yb = dls.one_batch(); xb,yb\n\n(tensor([[[ 5.7587,  5.8145,  5.6909,  ..., -1.7350, -1.0917, -0.3039]],\n \n         [[ 0.2592,  0.6001,  0.9024,  ...,  0.0878,  0.7782,  1.2575]],\n \n         [[ 0.2450,  0.6713,  1.1601,  ...,  1.6509,  1.9004,  2.0456]],\n \n         ...,\n \n         [[ 0.9055,  1.9541,  2.8322,  ..., -0.5598, -0.0746,  0.3829]],\n \n         [[-5.1401, -5.1290, -5.1660,  ..., -1.1601, -0.3699,  0.4581]],\n \n         [[ 0.7476,  0.7960,  0.9922,  ...,  1.7452,  1.2210,  0.5381]]],\n        device='cuda:0'),\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1.], device='cuda:0'))\n\n\n\nplt.plot(xb[25][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nlearn = ts_learner(dls, InceptionTime, loss_func=CustomLoss(), metrics=[mae, rmse], wd=0.01, cbs=ShowGraph())\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0002754228771664202)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae\n_rmse\ntime\n\n\n\n\n0\n0.064472\n0.203931\n0.310527\n0.311986\n00:21\n\n\n1\n0.023797\n0.260448\n0.376841\n0.379547\n00:21\n\n\n2\n0.013522\n0.197043\n0.302270\n0.303012\n00:21\n\n\n3\n0.007995\n0.191926\n0.295992\n0.296411\n00:21\n\n\n4\n0.005808\n0.183075\n0.284699\n0.285396\n00:21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nye_valid, y_valid = learn.get_preds()\n\n\n\n\n\n\n\n\n\nye_valid\n\nTensorBase([[1.0327],\n        [1.0194],\n        [1.0147],\n        ...,\n        [1.0063],\n        [1.0032],\n        [1.0098]])\n\n\n\ny_valid\n\ntorch.Size([9861])"
  },
  {
    "objectID": "generation.cnn_old.html",
    "href": "generation.cnn_old.html",
    "title": "generation.cnn",
    "section": "",
    "text": "from monitosed.data.core import *\nfrom monitosed.models import *\nfrom monitosed.losses import *\nimport mat73\nfrom fastcore.xtras import Path\nimport pandas as pd\nimport numpy as np\n\nfrom tsai.all import *"
  },
  {
    "objectID": "generation.cnn_old.html#load-data",
    "href": "generation.cnn_old.html#load-data",
    "title": "generation.cnn",
    "section": "Load data",
    "text": "Load data\n\nmats = load_mats(path)\n\n\n\n\nLoading: VR20_Reststim_data_clean.mat\nLoading: VR23_RestStim_data_clean.mat\nLoading: VR26_Reststim_data_clean.mat\nLoading: VR27_Reststim_data_clean.mat\nLoading: VR31_Reststim_data_clean.mat\nLoading: VR35_RestStim_data_clean.mat\nLoading: VR38_RestStim_data_clean.mat\nLoading: VR40_RestStim_data_clean.mat\nLoading: VR41_RestStim_data_clean.mat\nLoading: VR51_RestStim_data_clean.mat\nLoading: VR52_RestStim_data_clean.mat\nLoading: VR57_Reststim_data_clean.mat\nLoading: VR59_Reststim_data_clean.mat\nLoading: VR60_Reststim_data_clean.mat\n\n\n\n#Optional\nsignal_len = 1500"
  },
  {
    "objectID": "generation.cnn_old.html#get-labels",
    "href": "generation.cnn_old.html#get-labels",
    "title": "generation.cnn",
    "section": "Get labels",
    "text": "Get labels\nThe first idea is to perform forecasting, i.e. predict the future signal. To do so, we have to cut the measured signal into a past and the future we would like to predict.\n/! the stimulus happens at 250\n\ntrn_len = 1000\npred_len = signal_len-trn_len\nvalid_pct=0.2"
  },
  {
    "objectID": "generation.cnn_old.html#datablock",
    "href": "generation.cnn_old.html#datablock",
    "title": "generation.cnn",
    "section": "DataBlock",
    "text": "DataBlock\n\ngetters = [ItemGetter(0), ItemGetter(1)]\n\n\ndata = np.concatenate([read_data(mat) for mat in mats])\ndata = data[:,:,:signal_len]\ndata.shape\n\n(778, 173, 1500)\n\n\n\nx = torch.from_numpy(np.concatenate(data, axis=0))[:,:trn_len].unsqueeze(1)\ny = torch.from_numpy(np.concatenate(data, axis=0))[:,trn_len:]\nx.shape, y.shape\n\n(torch.Size([134594, 1, 1000]), torch.Size([134594, 500]))\n\n\n\nsplits = create_splits(mats)\n\n\n# For randperm\ntrain_ix = int((1-valid_pct)*len(mats))\nread_mats = [read_data(mat) for mat in mats]\nrearranged_mats = [new_rearrange(mat) for mat in read_mats]\nn_train_sample = np.concatenate(rearranged_mats[:train_ix]).shape[0]\nrandperm = torch.randperm(x.shape[0])\nrand_splits = (np.array(randperm[:n_train_sample]), np.array(randperm[n_train_sample:]))\n\n\nsplits[0].shape, splits[1].shape\n\n((103454,), (31140,))\n\n\n\n#dblock = DataBlock(blocks=(TSTensorBlock, TSTensorBlock),\n#                   getters=getters,\n#                   splitter=IndexSplitter(splits[1]),\n#                   item_tfms=None,\n#                   batch_tfms=None)\n\n\n#source = itemify(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n\n\ndls = get_ts_dls(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), splits=splits, bs=128)\n\n\n#dls = dblock.dataloaders(source, bs=64, val_bs=128, num_workers=4)\n\nKeyboardInterrupt: \n\n\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([128, 1, 1000]), torch.Size([128, 500]))\n\n\n\nplt.plot(xb[0][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nmodel = TimeSeriesModel(1, pred_len)\nlearn = Learner(dls, model, loss_func=CustomLoss(), metrics=[mae, rmse], wd=0.001, cbs=ShowGraph())\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.001737800776027143)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(15, lr_max=1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae\n_rmse\ntime\n\n\n\n\n0\n9.693105\n7.056831\n2.404412\n3.421878\n00:15\n\n\n1\n8.114635\n7.393760\n2.469032\n3.509770\n00:15\n\n\n2\n7.606911\n7.418622\n2.473575\n3.516201\n00:15\n\n\n3\n6.844526\n7.436425\n2.479528\n3.520416\n00:15\n\n\n4\n6.479427\n7.425214\n2.478462\n3.517380\n00:15\n\n\n5\n6.107447\n7.504903\n2.492799\n3.537939\n00:15\n\n\n6\n5.685992\n7.499231\n2.495255\n3.535986\n00:15\n\n\n7\n5.541937\n7.450821\n2.489485\n3.523090\n00:15\n\n\n8\n5.185974\n7.479107\n2.493871\n3.530487\n00:15\n\n\n9\n5.003172\n7.503276\n2.497432\n3.536824\n00:15\n\n\n10\n4.740528\n7.528675\n2.503478\n3.543144\n00:15\n\n\n11\n4.657327\n7.508465\n2.498060\n3.538201\n00:15\n\n\n12\n4.607917\n7.494348\n2.495207\n3.534612\n00:15\n\n\n13\n4.484475\n7.512960\n2.498985\n3.539340\n00:15\n\n\n14\n4.478148\n7.523282\n2.501396\n3.541915\n00:15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nin_valid, pred_valid, true_valid = learn.get_preds(with_input=True)\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(ncols=3, nrows=3, figsize=(12,8), dpi=150)\nfor i, ax in enumerate(axes.flat):\n    plot_idx = np.random.choice(np.arange(0, len(in_valid)))\n    true = np.concatenate([in_valid.numpy()[plot_idx,-1,:].reshape(-1), true_valid.numpy()[plot_idx,:].reshape(-1)])\n    pred = np.concatenate([in_valid.numpy()[plot_idx,-1,:].reshape(-1), pred_valid[plot_idx,:].reshape(-1)])\n    ax.plot(pred, color='red', label='preds')\n    ax.plot(true, color='green', label='true')\n    ax.vlines(trn_len-1, np.min(true), np.max(true), color='black')\n    if i == 0: ax.legend()\nfig.tight_layout();"
  },
  {
    "objectID": "data.html#test-the-dls-creation",
    "href": "data.html#test-the-dls-creation",
    "title": "data",
    "section": "Test the dls creation",
    "text": "Test the dls creation\n…\n\n\ncreate_dls_eoec\n\n create_dls_eoec (x, y, bs)",
    "crumbs": [
      "data",
      "data"
    ]
  },
  {
    "objectID": "data.html#test-the-dls_eoec-creation",
    "href": "data.html#test-the-dls_eoec-creation",
    "title": "data",
    "section": "Test the dls_eoec creation",
    "text": "Test the dls_eoec creation\n…",
    "crumbs": [
      "data",
      "data"
    ]
  },
  {
    "objectID": "regression.core_old.html",
    "href": "regression.core_old.html",
    "title": "classification.core",
    "section": "",
    "text": "users, labels = get_users_labels('../_data/Smarthy2_Behavioral.xlsx', drop_ixs=[0,2,7, 12, 13, 16])"
  },
  {
    "objectID": "classification.cnn_old.html",
    "href": "classification.cnn_old.html",
    "title": "classification.cnn",
    "section": "",
    "text": "from monitosed.classification.core import *\nfrom monitosed.data.core import *\nfrom monitosed.models import *\n\nfrom fastcore.xtras import Path\nfrom tsai.all import *\nfrom itertools import repeat\nfrom IPython.display import clear_output\n\n\nusers, labels = get_users_labels('../_data/Smarthy2_Behavioral.xlsx', drop_ixs=[0,2,7, 12, 13, 16])\n\n\npath = Path(\"../_data/foot1\"); path.ls()\n\n(#3) [Path('../_data/foot1/Rest'),Path('../_data/foot1/VR'),Path('../_data/foot1/VR34_VRstim_data_clean.mat')]\n\n\n\nmats_rest = load_mats(path/\"Rest\")\nmats_vr = load_mats(path/\"VR\")\n\n\n\n\nLoading: VR20_Reststim_data_clean.mat\nLoading: VR23_RestStim_data_clean.mat\nLoading: VR26_Reststim_data_clean.mat\nLoading: VR27_Reststim_data_clean.mat\nLoading: VR31_Reststim_data_clean.mat\nLoading: VR35_RestStim_data_clean.mat\nLoading: VR38_RestStim_data_clean.mat\nLoading: VR40_RestStim_data_clean.mat\nLoading: VR41_RestStim_data_clean.mat\nLoading: VR51_RestStim_data_clean.mat\nLoading: VR52_RestStim_data_clean.mat\nLoading: VR57_Reststim_data_clean.mat\nLoading: VR59_Reststim_data_clean.mat\nLoading: VR60_Reststim_data_clean.mat\nLoading: VR20_VRstim_data_clean.mat\nLoading: VR23_VRStim_data_clean.mat\nLoading: VR26_VRStim_data_clean.mat\nLoading: VR27_VRstim_data_clean.mat\nLoading: VR31_VRstim_data_clean.mat\nLoading: VR35_VRStim_data_clean.mat\nLoading: VR38_VRStim_data_clean.mat\nLoading: VR40_VRstim_data_clean.mat\nLoading: VR41_VRStim_data_clean.mat\nLoading: VR51_VRStim_data_clean.mat\nLoading: VR52_VRStim_data_clean.mat\nLoading: VR57_VRstim_data_clean.mat\nLoading: VR59_VRstim_data_clean.mat\nLoading: VR60_VRstim_data_clean.mat\n\n\n\n\n\nnp.stack(mats_rest[-1][‘data_clean’][‘trial’], axis=0).shape[0]+np.stack(mats_rest[-2][‘data_clean’][‘trial’], axis=0).shape[0]\nnp.stack(mats_vr[-1][‘data_clean’][‘trial’], axis=0).shape[0]+np.stack(mats_vr[-2][‘data_clean’][‘trial’], axis=0).shape[0]\n\nn_fold = len(mats_rest)/2\n\n\ndef run_kfold(arch, k, n_epochs=10, lr=1e-3):\n    accs = []\n\n    for i in range(int(n_fold)):\n        #model = StagerNet(channels=173, embed_dim=2)\n       \n        print(f'Fold {i}')\n\n        data_rest = np.concatenate([read_data(mat) for mat in np.delete(mats_rest, np.arange(2*i, 2*i+2))])\n        data_vr = np.concatenate([read_data(mat) for mat in np.delete(mats_rest, np.arange(2*i, 2*i+2))])\n\n        print(f'Len of data rest: {len(data_rest)}')\n        print(f'Len of data VR: {len(data_vr)}')\n\n        data_rest_v = np.concatenate([read_data(mat) for mat in mats_rest[2*i:2*i+2]])\n        data_vr_v = np.concatenate([read_data(mat) for mat in mats_vr[2*i:2*i+2]])\n\n        print(f'Len of validation data rest: {len(data_rest_v)}')\n        print(f'Len of validation data VR: {len(data_vr_v)}')\n\n        X = torch.cat([torch.Tensor(data_rest), torch.Tensor(data_vr)])\n        X_v = torch.cat([torch.Tensor(data_rest_v), torch.Tensor(data_vr_v)])\n\n        y = list(repeat(0., data_rest.shape[0]))+list(repeat(1., data_vr.shape[0]))\n        y_v = list(repeat(0., data_rest_v.shape[0]))+list(repeat(1., data_vr_v.shape[0]))\n\n        tfms = [None, Categorize()]\n\n        dsets_train = TSDatasets(X, y, tfms=tfms)\n        dsets_valid = TSDatasets(X_v, y_v, tfms=tfms)\n\n        dls = TSDataLoaders.from_dsets(dsets_train,\n                                   dsets_valid,\n                                   bs=[64, 128],\n                                   batch_tfms=[TSNormalize(), TSToGASF(cmap='summer')],\n                                   shuffle=True)\n        \n        model = create_model(arch, dls=dls, **k)\n\n\n        learn = ts_learner(dls, model, wd=1, metrics=accuracy)\n\n        learn.fit_one_cycle(n_epochs, lr)\n\n        accs.append(learn.recorder.values[-1][-1])\n        \n    return accs\n\n\nstgnet = StagerNet(channels=173, embed_dim=2)\n\n\naccs = run_kfold(n_epochs=30, lr=1e-3)\n\nFold 0\nLen of data rest: 666\nLen of data VR: 666\nLen of validation data rest: 112\nLen of validation data VR: 114\nFold 1\nLen of data rest: 663\nLen of data VR: 663\nLen of validation data rest: 115\nLen of validation data VR: 117\nFold 2\nLen of data rest: 669\nLen of data VR: 669\nLen of validation data rest: 109\nLen of validation data VR: 110\nFold 3\nLen of data rest: 670\nLen of data VR: 670\nLen of validation data rest: 108\nLen of validation data VR: 111\nFold 4\nLen of data rest: 678\nLen of data VR: 678\nLen of validation data rest: 100\nLen of validation data VR: 106\nFold 5\nLen of data rest: 664\nLen of data VR: 664\nLen of validation data rest: 114\nLen of validation data VR: 114\nFold 6\nLen of data rest: 658\nLen of data VR: 658\nLen of validation data rest: 120\nLen of validation data VR: 123\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.702421\n0.695994\n0.504425\n00:04\n\n\n1\n0.701286\n0.698188\n0.504425\n00:04\n\n\n2\n0.702113\n0.700154\n0.495575\n00:04\n\n\n3\n0.702188\n1.455414\n0.495575\n00:04\n\n\n4\n0.702619\n0.836042\n0.504425\n00:04\n\n\n5\n0.702225\n0.809931\n0.486726\n00:04\n\n\n6\n0.703992\n0.713080\n0.500000\n00:04\n\n\n7\n0.702361\n0.692285\n0.513274\n00:04\n\n\n8\n0.702203\n0.704703\n0.500000\n00:04\n\n\n9\n0.702266\n0.697316\n0.504425\n00:04\n\n\n10\n0.701039\n0.691921\n0.526549\n00:04\n\n\n11\n0.699699\n0.700926\n0.504425\n00:04\n\n\n12\n0.698996\n0.693802\n0.491150\n00:04\n\n\n13\n0.697979\n0.691042\n0.522124\n00:04\n\n\n14\n0.696966\n0.692313\n0.504425\n00:04\n\n\n15\n0.696966\n0.691706\n0.561947\n00:04\n\n\n16\n0.696405\n0.695152\n0.455752\n00:04\n\n\n17\n0.696080\n0.694191\n0.504425\n00:04\n\n\n18\n0.695984\n0.692799\n0.495575\n00:04\n\n\n19\n0.696295\n0.693860\n0.504425\n00:04\n\n\n20\n0.696610\n0.691729\n0.615044\n00:04\n\n\n21\n0.695703\n0.692805\n0.513274\n00:04\n\n\n22\n0.695050\n0.692189\n0.584071\n00:04\n\n\n23\n0.694538\n0.692527\n0.557522\n00:04\n\n\n24\n0.694260\n0.693103\n0.522124\n00:04\n\n\n25\n0.694090\n0.692472\n0.504425\n00:04\n\n\n26\n0.693822\n0.692468\n0.504425\n00:04\n\n\n27\n0.693675\n0.692480\n0.495575\n00:04\n\n\n28\n0.693540\n0.692489\n0.500000\n00:04\n\n\n29\n0.693406\n0.692493\n0.500000\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.705362\n0.695391\n0.495690\n00:04\n\n\n1\n0.703890\n0.696072\n0.504310\n00:04\n\n\n2\n0.702967\n0.690503\n0.573276\n00:04\n\n\n3\n0.702771\n0.835396\n0.495690\n00:04\n\n\n4\n0.702550\n0.830462\n0.495690\n00:04\n\n\n5\n0.702506\n0.980396\n0.504310\n00:04\n\n\n6\n0.703779\n0.706427\n0.456897\n00:04\n\n\n7\n0.702726\n0.712891\n0.495690\n00:04\n\n\n8\n0.701285\n0.681614\n0.500000\n00:04\n\n\n9\n0.700513\n0.691622\n0.491379\n00:04\n\n\n10\n0.699458\n0.693044\n0.534483\n00:04\n\n\n11\n0.698818\n0.692982\n0.543103\n00:04\n\n\n12\n0.697972\n0.679437\n0.495690\n00:04\n\n\n13\n0.698998\n0.722605\n0.504310\n00:04\n\n\n14\n0.698414\n0.690077\n0.495690\n00:04\n\n\n15\n0.698309\n0.689105\n0.495690\n00:04\n\n\n16\n0.698019\n0.690534\n0.495690\n00:04\n\n\n17\n0.697574\n0.695121\n0.487069\n00:04\n\n\n18\n0.697652\n0.692056\n0.495690\n00:04\n\n\n19\n0.696638\n0.691193\n0.607759\n00:04\n\n\n20\n0.695961\n0.695159\n0.500000\n00:04\n\n\n21\n0.695872\n0.692417\n0.495690\n00:04\n\n\n22\n0.695112\n0.694647\n0.487069\n00:04\n\n\n23\n0.694626\n0.694427\n0.474138\n00:04\n\n\n24\n0.694257\n0.693343\n0.530172\n00:04\n\n\n25\n0.693908\n0.692702\n0.530172\n00:04\n\n\n26\n0.693682\n0.692541\n0.491379\n00:04\n\n\n27\n0.693536\n0.692495\n0.491379\n00:04\n\n\n28\n0.693447\n0.692542\n0.491379\n00:04\n\n\n29\n0.693391\n0.692546\n0.491379\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.702109\n0.700234\n0.502283\n00:04\n\n\n1\n0.700991\n0.708766\n0.502283\n00:04\n\n\n2\n0.703673\n0.705469\n0.356164\n00:04\n\n\n3\n0.704165\n0.688191\n0.589041\n00:04\n\n\n4\n0.704845\n0.726403\n0.616438\n00:04\n\n\n5\n0.704729\n0.736976\n0.424658\n00:04\n\n\n6\n0.704563\n0.684895\n0.538813\n00:04\n\n\n7\n0.704592\n0.733078\n0.502283\n00:04\n\n\n8\n0.702183\n0.706152\n0.497717\n00:04\n\n\n9\n0.701236\n0.699713\n0.506849\n00:04\n\n\n10\n0.700172\n0.699485\n0.484018\n00:04\n\n\n11\n0.699017\n0.698990\n0.484018\n00:04\n\n\n12\n0.698100\n0.698093\n0.333333\n00:04\n\n\n13\n0.697331\n0.695976\n0.488584\n00:04\n\n\n14\n0.696824\n0.692744\n0.497717\n00:04\n\n\n15\n0.697570\n0.699137\n0.502283\n00:04\n\n\n16\n0.696838\n0.693033\n0.497717\n00:04\n\n\n17\n0.696830\n0.693787\n0.502283\n00:04\n\n\n18\n0.696250\n0.689165\n0.502283\n00:04\n\n\n19\n0.696249\n0.696960\n0.502283\n00:04\n\n\n20\n0.695764\n0.694301\n0.461187\n00:04\n\n\n21\n0.695467\n0.691431\n0.511416\n00:04\n\n\n22\n0.694932\n0.694517\n0.479452\n00:04\n\n\n23\n0.695127\n0.697800\n0.502283\n00:04\n\n\n24\n0.694679\n0.694527\n0.442922\n00:04\n\n\n25\n0.694280\n0.694589\n0.447489\n00:04\n\n\n26\n0.694017\n0.694186\n0.447489\n00:04\n\n\n27\n0.693768\n0.694543\n0.429224\n00:04\n\n\n28\n0.693618\n0.694589\n0.415525\n00:04\n\n\n29\n0.693515\n0.694574\n0.406393\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.702449\n0.693760\n0.506849\n00:04\n\n\n1\n0.701047\n0.694500\n0.424658\n00:04\n\n\n2\n0.701590\n0.732912\n0.497717\n00:04\n\n\n3\n0.707960\n0.741218\n0.506849\n00:04\n\n\n4\n0.708563\n0.746860\n0.493151\n00:04\n\n\n5\n0.705459\n0.886797\n0.506849\n00:04\n\n\n6\n0.702254\n0.769776\n0.493151\n00:04\n\n\n7\n0.700919\n0.729812\n0.493151\n00:04\n\n\n8\n0.703937\n0.686278\n0.493151\n00:04\n\n\n9\n0.702285\n0.695131\n0.497717\n00:04\n\n\n10\n0.700765\n0.690257\n0.520548\n00:04\n\n\n11\n0.700668\n0.685773\n0.493151\n00:04\n\n\n12\n0.699680\n0.696562\n0.442922\n00:04\n\n\n13\n0.698984\n0.703542\n0.506849\n00:04\n\n\n14\n0.697741\n0.679054\n0.525114\n00:04\n\n\n15\n0.697464\n0.697994\n0.506849\n00:04\n\n\n16\n0.697144\n0.694326\n0.497717\n00:04\n\n\n17\n0.696561\n0.689429\n0.493151\n00:04\n\n\n18\n0.696457\n0.697427\n0.506849\n00:04\n\n\n19\n0.695993\n0.694517\n0.470320\n00:04\n\n\n20\n0.695589\n0.695398\n0.506849\n00:04\n\n\n21\n0.695005\n0.692362\n0.575342\n00:04\n\n\n22\n0.694587\n0.693475\n0.502283\n00:04\n\n\n23\n0.694308\n0.693878\n0.461187\n00:04\n\n\n24\n0.694001\n0.691688\n0.511416\n00:04\n\n\n25\n0.693874\n0.690910\n0.493151\n00:04\n\n\n26\n0.693730\n0.692194\n0.493151\n00:04\n\n\n27\n0.693588\n0.692694\n0.497717\n00:04\n\n\n28\n0.693485\n0.692998\n0.497717\n00:04\n\n\n29\n0.693362\n0.693042\n0.502283\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.701843\n0.688055\n0.514563\n00:04\n\n\n1\n0.702267\n0.686159\n0.519417\n00:04\n\n\n2\n0.703704\n0.697039\n0.514563\n00:04\n\n\n3\n0.702302\n1.077913\n0.485437\n00:04\n\n\n4\n0.705226\n1.752952\n0.514563\n00:04\n\n\n5\n0.705995\n0.876490\n0.446602\n00:04\n\n\n6\n0.705504\n0.686981\n0.582524\n00:04\n\n\n7\n0.705263\n0.721490\n0.427184\n00:04\n\n\n8\n0.703035\n0.689355\n0.514563\n00:04\n\n\n9\n0.702967\n0.695125\n0.441748\n00:04\n\n\n10\n0.701922\n0.687943\n0.601942\n00:04\n\n\n11\n0.700365\n0.693493\n0.514563\n00:04\n\n\n12\n0.699007\n0.706160\n0.485437\n00:04\n\n\n13\n0.698584\n0.691846\n0.567961\n00:04\n\n\n14\n0.697725\n0.689283\n0.635922\n00:04\n\n\n15\n0.696725\n0.692509\n0.514563\n00:04\n\n\n16\n0.696577\n0.689832\n0.514563\n00:04\n\n\n17\n0.696692\n0.691606\n0.485437\n00:04\n\n\n18\n0.695923\n0.689485\n0.514563\n00:04\n\n\n19\n0.695909\n0.691038\n0.514563\n00:04\n\n\n20\n0.695430\n0.690500\n0.635922\n00:04\n\n\n21\n0.695100\n0.689937\n0.529126\n00:04\n\n\n22\n0.694596\n0.690664\n0.723301\n00:04\n\n\n23\n0.694301\n0.691148\n0.538835\n00:04\n\n\n24\n0.694045\n0.691076\n0.519417\n00:04\n\n\n25\n0.693809\n0.691580\n0.519417\n00:04\n\n\n26\n0.693631\n0.691785\n0.626214\n00:04\n\n\n27\n0.693483\n0.691729\n0.640777\n00:04\n\n\n28\n0.693378\n0.691685\n0.597087\n00:04\n\n\n29\n0.693300\n0.691685\n0.592233\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.702997\n0.697219\n0.500000\n00:04\n\n\n1\n0.701350\n0.694665\n0.473684\n00:04\n\n\n2\n0.703098\n0.735154\n0.500000\n00:04\n\n\n3\n0.705869\n0.705668\n0.473684\n00:04\n\n\n4\n0.706337\n0.751872\n0.495614\n00:04\n\n\n5\n0.707048\n0.699203\n0.504386\n00:04\n\n\n6\n0.705394\n0.706065\n0.508772\n00:04\n\n\n7\n0.704213\n0.753733\n0.500000\n00:04\n\n\n8\n0.705419\n0.695870\n0.500000\n00:04\n\n\n9\n0.703452\n0.701260\n0.500000\n00:04\n\n\n10\n0.703303\n0.689038\n0.530702\n00:04\n\n\n11\n0.702561\n0.712855\n0.500000\n00:04\n\n\n12\n0.701138\n0.727634\n0.500000\n00:04\n\n\n13\n0.700200\n0.695763\n0.500000\n00:04\n\n\n14\n0.699595\n0.705400\n0.500000\n00:04\n\n\n15\n0.699008\n0.695269\n0.451754\n00:04\n\n\n16\n0.697815\n0.691670\n0.548246\n00:04\n\n\n17\n0.697379\n0.697443\n0.500000\n00:04\n\n\n18\n0.696763\n0.693031\n0.500000\n00:04\n\n\n19\n0.696098\n0.691666\n0.600877\n00:04\n\n\n20\n0.695442\n0.691016\n0.548246\n00:04\n\n\n21\n0.695123\n0.690330\n0.539474\n00:04\n\n\n22\n0.694699\n0.691418\n0.605263\n00:04\n\n\n23\n0.694396\n0.692437\n0.508772\n00:04\n\n\n24\n0.694054\n0.692412\n0.504386\n00:04\n\n\n25\n0.693834\n0.692077\n0.587719\n00:04\n\n\n26\n0.693676\n0.691936\n0.596491\n00:04\n\n\n27\n0.693542\n0.691941\n0.596491\n00:04\n\n\n28\n0.693415\n0.692007\n0.614035\n00:04\n\n\n29\n0.693341\n0.692005\n0.609649\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.704846\n0.692812\n0.493827\n00:04\n\n\n1\n0.703574\n0.693361\n0.485597\n00:04\n\n\n2\n0.704332\n0.692235\n0.506173\n00:04\n\n\n3\n0.706557\n0.926075\n0.506173\n00:04\n\n\n4\n0.706899\n0.773525\n0.477366\n00:04\n\n\n5\n0.710077\n0.677513\n0.547325\n00:04\n\n\n6\n0.708971\n0.712915\n0.506173\n00:04\n\n\n7\n0.706644\n0.723837\n0.493827\n00:04\n\n\n8\n0.704705\n0.701263\n0.497942\n00:04\n\n\n9\n0.702386\n0.691429\n0.506173\n00:04\n\n\n10\n0.703270\n0.696334\n0.510288\n00:04\n\n\n11\n0.701397\n0.695224\n0.493827\n00:04\n\n\n12\n0.700435\n0.695506\n0.506173\n00:04\n\n\n13\n0.699108\n0.696166\n0.493827\n00:04\n\n\n14\n0.698108\n0.695509\n0.510288\n00:04\n\n\n15\n0.697416\n0.693331\n0.485597\n00:04\n\n\n16\n0.696899\n0.694771\n0.444444\n00:04\n\n\n17\n0.696432\n0.696039\n0.502058\n00:04\n\n\n18\n0.696620\n0.693769\n0.493827\n00:04\n\n\n19\n0.696653\n0.694761\n0.460905\n00:04\n\n\n20\n0.696592\n0.695613\n0.497942\n00:04\n\n\n21\n0.695688\n0.694196\n0.456790\n00:04\n\n\n22\n0.695147\n0.693994\n0.489712\n00:04\n\n\n23\n0.694675\n0.694014\n0.481481\n00:04\n\n\n24\n0.694354\n0.694421\n0.444444\n00:04\n\n\n25\n0.694013\n0.694222\n0.481481\n00:04\n\n\n26\n0.693779\n0.694085\n0.497942\n00:04\n\n\n27\n0.693590\n0.694183\n0.485597\n00:04\n\n\n28\n0.693463\n0.694179\n0.469136\n00:04\n\n\n29\n0.693358\n0.694189\n0.465021\n00:04\n\n\n\n\n\n\naccs\n\n[0.5,\n 0.4913793206214905,\n 0.4063926935195923,\n 0.5022830963134766,\n 0.5922330021858215,\n 0.609649121761322,\n 0.4650205671787262]\n\n\n\nnp.mean(accs), np.std(accs)\n\n(0.5095654002257756, 0.06541602644789596)\n\n\n\ni=0\n\n\ndata_rest = np.concatenate([read_data(mat) for mat in np.delete(mats_rest, np.arange(2*i, 2*i+2))])\n        data_vr = np.concatenate([read_data(mat) for mat in np.delete(mats_rest, np.arange(2*i, 2*i+2))])\n\n        print(f'Len of data rest: {len(data_rest)}')\n        print(f'Len of data VR: {len(data_vr)}')\n\n        data_rest_v = np.concatenate([read_data(mat) for mat in mats_rest[2*i:2*i+2]])\n        data_vr_v = np.concatenate([read_data(mat) for mat in mats_vr[2*i:2*i+2]])\n\n        print(f'Len of validation data rest: {len(data_rest_v)}')\n        print(f'Len of validation data VR: {len(data_vr_v)}')\n\n        X = torch.cat([torch.Tensor(data_rest), torch.Tensor(data_vr)])\n        X_v = torch.cat([torch.Tensor(data_rest_v), torch.Tensor(data_vr_v)])\n\n        y = list(repeat(0., data_rest.shape[0]))+list(repeat(1., data_vr.shape[0]))\n        y_v = list(repeat(0., data_rest_v.shape[0]))+list(repeat(1., data_vr_v.shape[0]))\n\n        tfms = [None, Categorize()]\n\n        dsets_train = TSDatasets(X, y, tfms=tfms)\n        dsets_valid = TSDatasets(X_v, y_v, tfms=tfms)\n\n        dls = TSDataLoaders.from_dsets(dsets_train,\n                                   dsets_valid,\n                                   bs=[64, 128],\n                                   batch_tfms=[TSNormalize(), TSToGASF(cmap='summer')],\n                                   shuffle=True)\n\n        learn = ts_learner(dls, stgnet, wd=1, metrics=accuracy)\n\n        learn.fit_one_cycle(10, 1e-4)\n\nLen of data rest: 666\nLen of data VR: 666\nLen of validation data rest: 112\nLen of validation data VR: 114\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.716926\n0.713491\n0.491150\n00:03\n\n\n1\n0.720461\n0.725461\n0.553097\n00:03\n\n\n2\n0.729884\n0.661030\n0.606195\n00:03\n\n\n3\n0.737751\n0.660194\n0.637168\n00:03\n\n\n4\n0.742483\n0.709350\n0.530973\n00:03\n\n\n5\n0.736911\n0.726843\n0.495575\n00:03\n\n\n6\n0.733057\n0.709912\n0.495575\n00:03\n\n\n7\n0.731455\n0.718297\n0.486726\n00:03\n\n\n8\n0.726464\n0.729451\n0.530973\n00:03\n\n\n9\n0.725285\n0.717606\n0.513274\n00:03\n\n\n\n\n\n\nlearn.recorder.values[-1][-1]\n\n0.5132743120193481\n\n\n\nstgnet = partial(StagerNet, channels=173, embed_dim=2)\n\n\nstgnet.__name__ = 'stgnet'\n\n\narchs = [(mWDN, {'levels': 4}), (FCN, {}), (ResNet, {}), (xresnet1d34, {}), (ResCNN, {}), (InceptionTime, {}), (XceptionTime, {}),\n         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}), \n         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}),\n         (LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False})]\n\nresults = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'mean accuracy', 'std accuracy', 'time'])\nfor i, (arch, k) in enumerate(archs):\n    \n    print(model.__class__.__name__)\n    accs = run_kfold(arch, k, n_epochs=30)\n    #learn = Learner(dls, model,  wd=0.1, metrics=accuracy)\n    start = time.time()\n    #learn.fit_one_cycle(30, 1e-3)\n    elapsed = time.time() - start\n    #vals = learn.recorder.values[-1]\n    results.loc[i] = [arch.__name__, k, count_parameters(model), np.mean(accs), np.std(accs), int(elapsed)]\n    results.sort_values(by='mean accuracy', ascending=False, kind='stable', ignore_index=True, inplace=True)\n    clear_output()\n    display(results)\n\n\n\n\n\n\n\n\narch\nhyperparams\ntotal params\nmean accuracy\nstd accuracy\ntime\n\n\n\n\n0\nmWDN\n{'levels': 4}\n6469942\n0.535218\n0.077116\n0\n\n\n1\nFCN\n{}\n6469942\n0.482316\n0.045060\n0\n\n\n\n\n\n\n\nmWDN\nFold 0\nLen of data rest: 666\nLen of data VR: 666\nLen of validation data rest: 112\nLen of validation data VR: 114\nFold 1\nLen of data rest: 663\nLen of data VR: 663\nLen of validation data rest: 115\nLen of validation data VR: 117\nFold 2\nLen of data rest: 669\nLen of data VR: 669\nLen of validation data rest: 109\nLen of validation data VR: 110\nFold 3\nLen of data rest: 670\nLen of data VR: 670\nLen of validation data rest: 108\nLen of validation data VR: 111\nFold 4\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.707200\n0.701268\n0.402655\n00:02\n\n\n1\n0.703355\n0.745684\n0.504425\n00:02\n\n\n2\n0.703672\n0.669767\n0.495575\n00:02\n\n\n3\n0.703532\n0.709207\n0.495575\n00:02\n\n\n4\n0.704467\n0.676107\n0.495575\n00:02\n\n\n5\n0.707008\n0.752433\n0.500000\n00:02\n\n\n6\n0.706082\n0.704095\n0.495575\n00:02\n\n\n7\n0.704355\n0.684261\n0.495575\n00:02\n\n\n8\n0.701501\n0.686508\n0.495575\n00:02\n\n\n9\n0.700122\n0.686363\n0.513274\n00:02\n\n\n10\n0.698892\n0.696161\n0.495575\n00:02\n\n\n11\n0.698424\n0.698052\n0.491150\n00:02\n\n\n12\n0.697715\n0.691382\n0.504425\n00:02\n\n\n13\n0.696120\n0.683805\n0.495575\n00:02\n\n\n14\n0.695960\n0.697070\n0.504425\n00:02\n\n\n15\n0.695562\n0.691002\n0.495575\n00:02\n\n\n16\n0.695179\n0.686288\n0.495575\n00:02\n\n\n17\n0.695154\n0.688773\n0.513274\n00:02\n\n\n18\n0.695546\n0.691643\n0.504425\n00:02\n\n\n19\n0.695129\n0.690913\n0.557522\n00:02\n\n\n20\n0.694672\n0.688998\n0.495575\n00:02\n\n\n21\n0.694289\n0.691027\n0.716814\n00:02\n\n\n22\n0.694036\n0.692353\n0.637168\n00:02\n\n\n23\n0.693896\n0.692860\n0.486726\n00:02\n\n\n24\n0.693684\n0.692004\n0.491150\n00:02\n\n\n25\n0.693523\n0.691803\n0.491150\n00:02\n\n\n26\n0.693424\n0.691488\n0.495575\n00:02\n\n\n27\n0.693336\n0.691673\n0.495575\n00:02\n\n\n28\n0.693332\n0.691698\n0.500000\n00:02\n\n\n29\n0.693286\n0.691744\n0.500000\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.701312\n0.688654\n0.500000\n00:02\n\n\n1\n0.701392\n0.679508\n0.508621\n00:02\n\n\n2\n0.704031\n0.696105\n0.500000\n00:02\n\n\n3\n0.704855\n0.689063\n0.495690\n00:02\n\n\n4\n0.705128\n0.713636\n0.504310\n00:02\n\n\n5\n0.705451\n0.666304\n0.521552\n00:02\n\n\n6\n0.705023\n0.703801\n0.495690\n00:02\n\n\n7\n0.703736\n0.683261\n0.849138\n00:02\n\n\n8\n0.701125\n0.699328\n0.478448\n00:02\n\n\n9\n0.699625\n0.699614\n0.504310\n00:02\n\n\n10\n0.699265\n0.691793\n0.521552\n00:02\n\n\n11\n0.698243\n0.698501\n0.495690\n00:02\n\n\n12\n0.698085\n0.692975\n0.495690\n00:02\n\n\n13\n0.696716\n0.696118\n0.504310\n00:02\n\n\n14\n0.696385\n0.697393\n0.495690\n00:02\n\n\n15\n0.696154\n0.696486\n0.504310\n00:02\n\n\n16\n0.695745\n0.692548\n0.500000\n00:02\n\n\n17\n0.695132\n0.691024\n0.512931\n00:02\n\n\n18\n0.694832\n0.691406\n0.603448\n00:02\n\n\n19\n0.694891\n0.693253\n0.504310\n00:02\n\n\n20\n0.694362\n0.692582\n0.495690\n00:02\n\n\n21\n0.694408\n0.691990\n0.538793\n00:02\n\n\n22\n0.694152\n0.690959\n0.525862\n00:02\n\n\n23\n0.694008\n0.691131\n0.512931\n00:02\n\n\n24\n0.693813\n0.691733\n0.607759\n00:02\n\n\n25\n0.693635\n0.692239\n0.573276\n00:02\n\n\n26\n0.693493\n0.692168\n0.655172\n00:02\n\n\n27\n0.693406\n0.692237\n0.629310\n00:02\n\n\n28\n0.693342\n0.692257\n0.637931\n00:02\n\n\n29\n0.693306\n0.692263\n0.633621\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.701420\n0.703869\n0.479452\n00:02\n\n\n1\n0.700859\n0.700426\n0.433790\n00:02\n\n\n2\n0.703134\n0.725723\n0.438356\n00:02\n\n\n3\n0.703719\n0.697979\n0.397260\n00:02\n\n\n4\n0.704392\n0.663118\n0.502283\n00:02\n\n\n5\n0.704931\n0.799299\n0.479452\n00:02\n\n\n6\n0.704164\n0.691390\n0.534247\n00:02\n\n\n7\n0.702477\n0.679886\n0.502283\n00:02\n\n\n8\n0.701891\n0.685603\n0.497717\n00:02\n\n\n9\n0.700315\n0.695544\n0.502283\n00:02\n\n\n10\n0.698466\n0.708580\n0.520548\n00:02\n\n\n11\n0.697624\n0.683548\n0.497717\n00:02\n\n\n12\n0.697327\n0.690109\n0.520548\n00:02\n\n\n13\n0.696858\n0.692917\n0.470320\n00:02\n\n\n14\n0.697124\n0.694180\n0.502283\n00:02\n\n\n15\n0.696563\n0.696277\n0.484018\n00:02\n\n\n16\n0.695876\n0.698357\n0.502283\n00:02\n\n\n17\n0.695412\n0.686228\n0.497717\n00:02\n\n\n18\n0.695390\n0.703401\n0.484018\n00:02\n\n\n19\n0.694856\n0.688293\n0.525114\n00:02\n\n\n20\n0.695180\n0.693022\n0.484018\n00:02\n\n\n21\n0.695081\n0.685958\n0.497717\n00:02\n\n\n22\n0.694670\n0.702360\n0.502283\n00:02\n\n\n23\n0.694321\n0.699435\n0.502283\n00:02\n\n\n24\n0.694068\n0.693388\n0.470320\n00:02\n\n\n25\n0.693942\n0.689235\n0.484018\n00:02\n\n\n26\n0.693743\n0.692295\n0.447489\n00:02\n\n\n27\n0.693552\n0.692783\n0.493151\n00:02\n\n\n28\n0.693467\n0.692380\n0.479452\n00:02\n\n\n29\n0.693397\n0.692491\n0.474886\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.705132\n0.678527\n0.753425\n00:02\n\n\n1\n0.702746\n0.687064\n0.488584\n00:02\n\n\n2\n0.705506\n0.677194\n0.515982\n00:02\n\n\n3\n0.705989\n0.675951\n0.666667\n00:02\n\n\n4\n0.706232\n0.763779\n0.506849\n00:02\n\n\n5\n0.705641\n0.680605\n0.675799\n00:02\n\n\n6\n0.705789\n0.699037\n0.506849\n00:02\n\n\n7\n0.703553\n0.707507\n0.506849\n00:02\n\n\n8\n0.702434\n0.693249\n0.493151\n00:02\n\n\n9\n0.700856\n0.683009\n0.520548\n00:02\n\n\n10\n0.699997\n0.695405\n0.497717\n00:02\n\n\n11\n0.698895\n0.690808\n0.497717\n00:02\n\n\n12\n0.698208\n0.694079\n0.506849\n00:02\n\n\n13\n0.697577\n0.702577\n0.264840\n00:02\n\n\n14\n0.696701\n0.697165\n0.506849\n00:02\n\n\n15\n0.696608\n0.694346\n0.470320\n00:02\n\n\n16\n0.696440\n0.701583\n0.264840\n00:02\n\n\n17\n0.695894\n0.697156\n0.342466\n00:02\n\n\n18\n0.695316\n0.693727\n0.493151\n00:02\n\n\n19\n0.695035\n0.690033\n0.794521\n00:02\n\n\n20\n0.694748\n0.692617\n0.493151\n00:02\n\n\n21\n0.694351\n0.693672\n0.497717\n00:02\n\n\n22\n0.694297\n0.695485\n0.506849\n00:02\n\n\n23\n0.694600\n0.694306\n0.493151\n00:02\n\n\n24\n0.694336\n0.696939\n0.465753\n00:02\n\n\n25\n0.693984\n0.697360\n0.251142\n00:02\n\n\n26\n0.693734\n0.697575\n0.255708\n00:02\n\n\n27\n0.693610\n0.697483\n0.232877\n00:02\n\n\n28\n0.693470\n0.697397\n0.242009\n00:02\n\n\n29\n0.693371\n0.697320\n0.242009\n00:02\n\n\n\n\n\n\nnp.mean(accs)\n\n0.5105760480676379\n\n\n\nnp.std(accs)\n\n0.10233512648204385\n\n\n\ndata_rest = np.concatenate([read_data(mat) for mat in mats_rest])\ndata_vr = np.concatenate([read_data(mat) for mat in mats_vr])\n\n\nX = torch.cat([torch.Tensor(data_rest), torch.Tensor(data_vr)])\n#X_v = torch.cat([torch.Tensor(data_rest_v), torch.Tensor(data_vr_v)])\n\n\ny = list(repeat(0., data_rest.shape[0]))+list(repeat(1., data_vr.shape[0]))\n#y_v = list(repeat(0., data_rest_v.shape[0]))+list(repeat(1., data_vr_v.shape[0]))\n\n\nnp.stack(mats_rest[-1]['data_clean']['trial'], axis=0).shape[0]+np.stack(mats_rest[-2]['data_clean']['trial'], axis=0).shape[0]\n\n120\n\n\n\nnp.stack(mats_vr[-1]['data_clean']['trial'], axis=0).shape[0]+np.stack(mats_vr[-2]['data_clean']['trial'], axis=0).shape[0]\n\n123\n\n\n\ntrain_ix = 1330\n\n\ntrain_split = np.concatenate([np.arange(0, len(data_rest)-120), np.arange(len(data_rest), len(X)-123)])\nval_split = np.concatenate([np.arange(len(data_rest)-120, len(data_rest)), np.arange(len(X)-123, len(X))])\nsplits = (train_split, val_split)\n\ntrain: 0-&gt;658 + 779-&gt;1450 valid: 659-&gt;778 + 1451-&gt;1573\n\ntfms = [None, Categorize()]\n\n\ndsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n#dsets_valid = TSDatasets(X_v, y_v, tfms=tfms)\n\n\ndls = TSDataLoaders.from_dsets(dsets.train,\n                               dsets.valid,\n                               bs=[64, 128],\n                               batch_tfms=[TSNormalize(), TSToGASF(cmap='summer')],\n                               shuffle=True)\n\n\nmodel = create_model(ResNet, dls=dls)\nlearn = Learner(dls, model,  metrics=accuracy)\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(30, 1e-2)\n\n\n\n\n\n\n    \n      \n      3.33% [1/30 00:02&lt;01:00]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.452104\n0.970099\n0.522634\n00:02\n\n\n\n\n\n    \n      \n      35.00% [7/20 00:00&lt;00:01 0.3665]\n    \n    \n\n\nKeyboardInterrupt: \n\n\n\nlearn.validate()\n\n\n\n\n\n\n\n\n(#2) [1.1288049221038818,0.5555555820465088]\n\n\n\nye_valid, y_valid = learn.get_preds()\n\n\n\n\n\n\n\n\n\nxb, yb = dls.one_batch()\n\n\nxb.shape\n\ntorch.Size([64, 173, 1500])\n\n\n\nplt.imshow(xb[0].cpu().numpy())\n\n\n\n\n\n\n\n\n\nfrom monitosed.models import *\n\n\nnet = StagerNet(173, embed_dim=2)\n\n\nlearn = ts_learner(dls, net, wd=1, metrics=accuracy, cbs=ShowGraph())\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00015848931798245758)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(30, lr_max=1e-4)\n\n\n\n\n\n\n    \n      \n      53.33% [16/30 00:56&lt;00:49]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.804757\n0.750243\n0.522634\n00:03\n\n\n1\n0.778671\n0.780791\n0.485597\n00:03\n\n\n2\n0.754107\n0.727186\n0.576132\n00:03\n\n\n3\n0.709645\n0.708419\n0.596708\n00:03\n\n\n4\n0.643204\n0.745287\n0.600823\n00:03\n\n\n5\n0.572034\n0.738180\n0.596708\n00:03\n\n\n6\n0.507973\n0.675054\n0.625514\n00:03\n\n\n7\n0.441709\n0.739956\n0.617284\n00:03\n\n\n8\n0.375905\n0.793724\n0.650206\n00:03\n\n\n9\n0.318464\n0.836842\n0.613169\n00:03\n\n\n10\n0.263602\n0.810791\n0.613169\n00:03\n\n\n11\n0.220051\n0.885209\n0.592593\n00:03\n\n\n12\n0.182570\n0.863165\n0.641975\n00:03\n\n\n13\n0.151466\n0.922131\n0.621399\n00:03\n\n\n14\n0.130070\n1.049765\n0.592593\n00:03\n\n\n15\n0.109526\n0.989459\n0.604938\n00:03\n\n\n\n\n\n    \n      \n      55.00% [11/20 00:01&lt;00:01 0.1025]\n    \n    \n\n\n\n\n\n\n\n\n\nKeyboardInterrupt: \n\n\n\n\n\n\n\n\n\n\narchs = [(mWDN, {'levels': 4}), (FCN, {}), (ResNet, {}), (xresnet1d34, {}), (ResCNN, {}), (InceptionTime, {}), (XceptionTime, {}),\n         (LSTM, {'n_layers':1, 'bidirectional': False}), (LSTM, {'n_layers':2, 'bidirectional': False}), (LSTM, {'n_layers':3, 'bidirectional': False}), \n         (LSTM, {'n_layers':1, 'bidirectional': True}), (LSTM, {'n_layers':2, 'bidirectional': True}),\n         (LSTM_FCN, {}), (LSTM_FCN, {'shuffle': False})]\n\nresults = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy', 'time'])\nfor i, (arch, k) in enumerate(archs):\n    model = create_model(arch, dls=dls, **k)\n    print(model.__class__.__name__)\n    learn = Learner(dls, model,  wd=0.1, metrics=accuracy)\n    start = time.time()\n    learn.fit_one_cycle(30, 1e-3)\n    elapsed = time.time() - start\n    vals = learn.recorder.values[-1]\n    results.loc[i] = [arch.__name__, k, count_parameters(model), vals[0], vals[1], vals[2], int(elapsed)]\n    results.sort_values(by='accuracy', ascending=False, kind='stable', ignore_index=True, inplace=True)\n    clear_output()\n    display(results)\n\n\n\n\n\n\n\n\narch\nhyperparams\ntotal params\ntrain loss\nvalid loss\naccuracy\ntime\n\n\n\n\n0\nmWDN\n{'levels': 4}\n6469942\n0.000196\n1.233036\n0.613169\n139\n\n\n1\nInceptionTime\n{}\n488130\n0.000382\n1.223705\n0.588477\n110\n\n\n2\nxresnet1d34\n{}\n7244770\n0.000140\n1.676917\n0.543210\n56\n\n\n3\nLSTM\n{'n_layers': 1, 'bidirectional': True}\n220402\n0.003864\n1.687751\n0.534979\n78\n\n\n4\nLSTM\n{'n_layers': 2, 'bidirectional': True}\n462002\n0.000970\n2.440332\n0.522634\n145\n\n\n5\nResNet\n{}\n566530\n0.000325\n1.574891\n0.506173\n61\n\n\n6\nLSTM_FCN\n{'shuffle': False}\n528634\n0.000746\n1.481025\n0.506173\n77\n\n\n7\nResCNN\n{}\n344323\n0.000476\n1.551743\n0.485597\n46\n\n\n8\nLSTM\n{'n_layers': 1, 'bidirectional': False}\n110202\n0.006093\n1.745541\n0.485597\n53\n\n\n9\nXceptionTime\n{}\n426740\n0.186315\n0.724546\n0.477366\n99\n\n\n10\nLSTM\n{'n_layers': 2, 'bidirectional': False}\n191002\n0.000888\n2.975230\n0.460905\n82\n\n\n11\nLSTM\n{'n_layers': 3, 'bidirectional': False}\n271802\n0.000375\n3.598920\n0.448560\n111\n\n\n12\nFCN\n{}\n418434\n0.000843\n1.505409\n0.436214\n39\n\n\n13\nLSTM_FCN\n{}\n1059434\n0.000694\n1.490152\n0.419753\n49\n\n\n\n\n\n\n\n\narchs = [(mWDN, {'levels': 4}), (ResNet, {}), (xresnet1d34, {}), (ResCNN, {}), (InceptionTime, {}), (XceptionTime, {}),\n         (LSTM, {'n_layers':1, 'bidirectional': False})]\n\n\nresults = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy', 'time'])\n\ntfms = [None, Categorize()]\nbts = [[TSNormalize(), TSToPlot()], \n       [TSNormalize(), TSToMat(cmap='viridis')],\n       [TSNormalize(), TSToGADF(cmap='spring')],\n       [TSNormalize(), TSToGASF(cmap='summer')],\n       [TSNormalize(), TSToMTF(cmap='autumn')],\n       [TSNormalize(), TSToRP(cmap='winter')]]\nbtns = ['Plot', 'Mat', 'GADF', 'GASF', 'MTF', 'RP']\nfor i, (bt, btn) in enumerate(zip(bts, btns)):\n    dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n    dls = TSDataLoaders.from_dsets(dsets.train,\n                                   dsets.valid,\n                                   bs=[64, 128],\n                                   batch_tfms=bt,\n                                   shuffle=True)\n    model = StagerNet(173, embed_dim=2)\n    print(model.__class__.__name__)\n    learn = Learner(dls, model,  wd=1, metrics=accuracy)\n    start = time.time()\n    learn.fit_one_cycle(30, 1e-3)\n    elapsed = time.time() - start\n    vals = learn.recorder.values[-1]\n    results.loc[i] = [btn, k, count_parameters(model), vals[0], vals[1], vals[2], int(elapsed)]\n    results.sort_values(by='accuracy', ascending=False, kind='stable', ignore_index=True, inplace=True)\n    clear_output()\n    display(results)\n\n\n\n\n\n\n\n\narch\nhyperparams\ntotal params\ntrain loss\nvalid loss\naccuracy\ntime\n\n\n\n\n0\nRP\n{'shuffle': False}\n65944\n0.000971\n2.140876\n0.683128\n103\n\n\n1\nGADF\n{'shuffle': False}\n65944\n0.000311\n1.834402\n0.650206\n103\n\n\n2\nPlot\n{'shuffle': False}\n65944\n0.000523\n2.371059\n0.646091\n103\n\n\n3\nMat\n{'shuffle': False}\n65944\n0.000429\n2.048433\n0.625514\n103\n\n\n4\nMTF\n{'shuffle': False}\n65944\n0.000306\n2.048115\n0.588477\n103\n\n\n5\nGASF\n{'shuffle': False}\n65944\n0.000439\n1.873248\n0.543210\n103\n\n\n\n\n\n\n\n\nresults = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy', 'time'])\n\ntfms = [None, Categorize()]\nbts = [[TSNormalize(), TSToPlot()], \n       [TSNormalize(), TSToMat(cmap='viridis')],\n       [TSNormalize(), TSToGADF(cmap='spring')],\n       [TSNormalize(), TSToGASF(cmap='summer')],\n       [TSNormalize(), TSToMTF(cmap='autumn')],\n       [TSNormalize(), TSToRP(cmap='winter')]]\nbtns = ['Plot', 'Mat', 'GADF', 'GASF', 'MTF', 'RP']\nfor i, (bt, btn) in enumerate(zip(bts, btns)):\n    dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n    dls = TSDataLoaders.from_dsets(dsets.train,\n                                   dsets.valid,\n                                   bs=[64, 128],\n                                   batch_tfms=bt,\n                                   shuffle=True)\n    model = create_model(mWDN, dls=dls, levels=4)\n    print(model.__class__.__name__)\n    learn = Learner(dls, model,  wd=0.1, metrics=accuracy)\n    start = time.time()\n    learn.fit_one_cycle(30, 1e-3)\n    elapsed = time.time() - start\n    vals = learn.recorder.values[-1]\n    results.loc[i] = [btn, k, count_parameters(model), vals[0], vals[1], vals[2], int(elapsed)]\n    results.sort_values(by='accuracy', ascending=False, kind='stable', ignore_index=True, inplace=True)\n    clear_output()\n    display(results)\n\n\n\n\n\n\n\n\narch\nhyperparams\ntotal params\ntrain loss\nvalid loss\naccuracy\ntime\n\n\n\n\n0\nGASF\n{'shuffle': False}\n6469942\n0.000174\n0.928896\n0.711934\n140\n\n\n1\nMat\n{'shuffle': False}\n6469942\n0.000182\n0.960350\n0.683128\n140\n\n\n2\nMTF\n{'shuffle': False}\n6469942\n0.000293\n1.008863\n0.679012\n140\n\n\n3\nPlot\n{'shuffle': False}\n6469942\n0.000244\n0.959720\n0.633745\n140\n\n\n4\nRP\n{'shuffle': False}\n6469942\n0.000288\n1.545471\n0.567901\n140\n\n\n5\nGADF\n{'shuffle': False}\n6469942\n0.000223\n1.665993\n0.534979\n140\n\n\n\n\n\n\n\n\nxb,yb = dls.one_batch()\n\n\nclass Hook():\n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n\n\nhook_output = Hook()\nhook = learn.model.conv3.register_forward_hook(hook_output.hook_func)\n\n\nwith torch.no_grad(): output = learn.model.eval()(xb)\n\n\nact = hook_output.stored[0]\n\n\nlearn.model\n\nStagerNet(\n  (conv1): Conv2d(1, 173, kernel_size=(173, 1), stride=(1, 1))\n  (conv2): Conv2d(1, 16, kernel_size=(1, 50), stride=(1, 1))\n  (conv3): Conv2d(16, 16, kernel_size=(1, 50), stride=(1, 1))\n  (linear1): Linear(in_features=11072, out_features=2, bias=True)\n  (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n\n\n\nxb.shape\n\ntorch.Size([64, 173, 1500])\n\n\n\nact.shape\n\ntorch.Size([16, 173, 62])\n\n\n\nlearn.model.conv3.weight.shape\n\ntorch.Size([16, 16, 1, 50])\n\n\n\nclass Hook():\n    def __init__(self, m):\n        self.hook = m.register_forward_hook(self.hook_func)   \n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\n\nclass HookBwd():\n    def __init__(self, m):\n        self.hook = m.register_backward_hook(self.hook_func)   \n    def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\n\ncls = 1\nwith HookBwd(learn.model.conv3) as hookg:\n    with Hook(learn.model.conv3) as hook:\n        output = learn.model.eval()(xb.cuda())\n        act = hook.stored\n    output[0,cls].backward()\n    grad = hookg.stored\n\n\nw = act[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\n\n_,ax = plt.subplots()\nax.imshow(xb[0].cpu().numpy())\nax.imshow(cam_map.detach().cpu(), alpha=0.6,\n              interpolation='bilinear', cmap='magma');\n\n\n\n\n\n\n\n\n\nmats = load_mats(path)\n\n\n\n\nLoading: VR23_RestStim_data_clean.mat\nLoading: VR20_Reststim_data_clean.mat\n\n\n\n#Optional\nsignal_len = 1500\n\n\nx = stack_trials(mats, signal_len)\ny = prepare_labels(mats, labels.values)\nx.shape, y.shape\n\n(torch.Size([19376, 1, 1500]), torch.Size([19376]))\n\n\n\nread_mats = [read_data(mat) for mat in mats]\nrearranged_mats = [new_rearrange(mat) for mat in read_mats]\n\n\nsplits = create_splits(mats)\n\n\ntfms  = [None, [TSRegression()]]\nbatch_tfms = TSStandardize(by_sample=True, by_var=True)\n#dls = get_ts_dls(x, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=128)\ndls = get_ts_dls(x, y, splits=splits, tfms=tfms, bs=128)\n\n\nxb,yb = dls.one_batch(); xb,yb\n\n(tensor([[[ 5.7587,  5.8145,  5.6909,  ..., -1.7350, -1.0917, -0.3039]],\n \n         [[ 0.2592,  0.6001,  0.9024,  ...,  0.0878,  0.7782,  1.2575]],\n \n         [[ 0.2450,  0.6713,  1.1601,  ...,  1.6509,  1.9004,  2.0456]],\n \n         ...,\n \n         [[ 0.9055,  1.9541,  2.8322,  ..., -0.5598, -0.0746,  0.3829]],\n \n         [[-5.1401, -5.1290, -5.1660,  ..., -1.1601, -0.3699,  0.4581]],\n \n         [[ 0.7476,  0.7960,  0.9922,  ...,  1.7452,  1.2210,  0.5381]]],\n        device='cuda:0'),\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1.], device='cuda:0'))\n\n\n\nplt.plot(xb[25][0].to('cpu').numpy())\n\n\n\n\n\n\n\n\n\nlearn = ts_learner(dls, InceptionTime, loss_func=CustomLoss(), metrics=[mae, rmse], wd=0.01, cbs=ShowGraph())\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0002754228771664202)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmae\n_rmse\ntime\n\n\n\n\n0\n0.064472\n0.203931\n0.310527\n0.311986\n00:21\n\n\n1\n0.023797\n0.260448\n0.376841\n0.379547\n00:21\n\n\n2\n0.013522\n0.197043\n0.302270\n0.303012\n00:21\n\n\n3\n0.007995\n0.191926\n0.295992\n0.296411\n00:21\n\n\n4\n0.005808\n0.183075\n0.284699\n0.285396\n00:21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = create_model(arch, dls=dls, **k)\nlearn = Learner(dls, model,  metrics=accuracy)\nlearn.fit_one_cycle(100, 1e-3)\n\n\nye_valid, y_valid = learn.get_preds()\n\nCould not do one pass in your dataloader, there is something wrong in it. Please see the stack trace below:\n\n\nRuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.92 GiB total capacity; 4.54 GiB already allocated; 63.44 MiB free; 4.76 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\n\n\nye_valid\n\nTensorBase([[1.0327],\n        [1.0194],\n        [1.0147],\n        ...,\n        [1.0063],\n        [1.0032],\n        [1.0098]])\n\n\n\ny_valid\n\ntorch.Size([9861])"
  }
]